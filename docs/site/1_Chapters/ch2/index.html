
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.2, mkdocs-material-7.2.6">
    
    
      
        <title>Chapter 2: Performing Data Structures - Scripted Performances</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.802231af.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.3f5d1f46.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    
      


    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="">
  
    
    <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
      <script>var palette=__get("__palette");if(null!==palette&&"object"==typeof palette.color)for(var key in palette.color)document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-2-performing-data-structures" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Scripted Performances" class="md-header__button md-logo" aria-label="Scripted Performances" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Scripted Performances
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Chapter 2: Performing Data Structures
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent=""  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M7 10a2 2 0 0 1 2 2 2 2 0 0 1-2 2 2 2 0 0 1-2-2 2 2 0 0 1 2-2m10-3a5 5 0 0 1 5 5 5 5 0 0 1-5 5H7a5 5 0 0 1-5-5 5 5 0 0 1 5-5h10M7 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3h10a3 3 0 0 0 3-3 3 3 0 0 0-3-3H7z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="red" data-md-color-accent=""  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/olliepalmer/phd/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Scripted Performances" class="md-nav__button md-logo" aria-label="Scripted Performances" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Scripted Performances
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/olliepalmer/phd/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../faqs/" class="md-nav__link">
        FAQs
      </a>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      <label class="md-nav__link" for="__nav_3">
        0 Front Matter
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="0 Front Matter" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          0 Front Matter
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../0_Front%20Matter/0.1-abstract/" class="md-nav__link">
        Abstract
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../0_Front%20Matter/0.2-acknowledgements/" class="md-nav__link">
        Acknowledgements
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../0_Front%20Matter/0.3-figures-and-tables/" class="md-nav__link">
        Figures and tables
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      <label class="md-nav__link" for="__nav_4">
        1 Chapters
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="1 Chapters" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          1 Chapters
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../ch0/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../ch1/" class="md-nav__link">
        Chapter 1: Diagramming the Absurd
      </a>
    </li>
  

          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Chapter 2: Performing Data Structures
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Chapter 2: Performing Data Structures
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#nybble-videos" class="md-nav__link">
    Nybble videos
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#scriptych-videos" class="md-nav__link">
    Scriptych videos
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#code-writing-and-code-thinking" class="md-nav__link">
    Code writing and code thinking
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#nybble" class="md-nav__link">
    Nybble
  </a>
  
    <nav class="md-nav" aria-label="Nybble">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#methodology" class="md-nav__link">
    Methodology
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#nybble-dance-instructions-sheet-21-september-2013" class="md-nav__link">
    Nybble dance instructions sheet: 21 September 2013
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#nybble-cheat-sheet-21-september-2013" class="md-nav__link">
    Nybble cheat sheet: 21 September 2013
  </a>
  
    <nav class="md-nav" aria-label="Nybble cheat sheet: 21 September 2013">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#soundtrack" class="md-nav__link">
    Soundtrack
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dancers" class="md-nav__link">
    Dancers
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#scriptych" class="md-nav__link">
    Scriptych
  </a>
  
    <nav class="md-nav" aria-label="Scriptych">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#design-process" class="md-nav__link">
    Design process
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#soundtrack_1" class="md-nav__link">
    Soundtrack
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance" class="md-nav__link">
    Performance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#film" class="md-nav__link">
    Film
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../ch3/" class="md-nav__link">
        Chapter 3: Noise and Difference
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../ch4/" class="md-nav__link">
        Chapter 4: Reflexive Scripted Design:
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../ch5/" class="md-nav__link">
        Conclusion
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../ch6/" class="md-nav__link">
        Bibliography
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      <label class="md-nav__link" for="__nav_5">
        2 Appendix
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="2 Appendix" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          2 Appendix
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../2_Appendix/7.3.1-godot-machine-development/" class="md-nav__link">
        Godot Machine technical development
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../2_Appendix/7.3.2-ant-ballet-development/" class="md-nav__link">
        Ant Ballet technical development
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/olliepalmer/phd/edit/master/docs/1_Chapters/ch2.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="chapter-2-performing-data-structures">Chapter 2: Performing Data Structures<a class="headerlink" href="#chapter-2-performing-data-structures" title="Permanent link">#</a></h1>
<h1><a href="#nybble">Nybble</a> and <a href="#scriptych">Scriptych</a></h1>

<hr />
<h2 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">#</a></h2>
<blockquote>
<p>Computation is central to the contemporary built environment, yet the underlying principles of computing are not widely known by the general public (as highlighted by recent debates in the media about advances in artificial intelligence research). This chapter presents a pair of novel design projects which use dancers to show computational processes in the form of diagrammatic performances. Nybble is a performance which acts as a diagram of John Searle's Chinese Room argument against hard artificial intelligence; Scriptych consists of dancers interacting with a three-dimensional database of words. Scripting is used as a mode of instruction for performers in both performances, with direct computer-scripted feedback provided via a novel interface in Scriptych. This advances the notion of script as mode of performance-instruction in the context of this thesis, as well as demonstrating computer scripting as a mode of interaction. It finds that the notion of performance-script can be used to drive performances at various stages the design process, representing different levels of agency for performers.</p>
</blockquote>
<hr />
<h2 id="nybble-videos">Nybble videos<a class="headerlink" href="#nybble-videos" title="Permanent link">#</a></h2>
<p>V&amp;A Museum | 21-22 September 2013</p>
<h3>Nybble: project video</h3>

<iframe src="http://player.vimeo.com/video/216712681" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>

<h3>Nybble: code base</h3>

<iframe src="http://player.vimeo.com/video/216712602" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>

<h3>Nybble: timelapse 1</h3>

<iframe src="http://player.vimeo.com/video/216712752" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>

<h3>Nybble: timelapse 2</h3>

<iframe src="http://player.vimeo.com/video/216712824" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>

<hr />
<h2 id="scriptych-videos">Scriptych videos<a class="headerlink" href="#scriptych-videos" title="Permanent link">#</a></h2>
<p>Opéra Garnier de Paris | 17-18 June 2016</p>
<h3>Scriptych: project video</h3>

<iframe src="http://player.vimeo.com/video/243660552" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>

<h3>Scriptych: making of (INA)</h3>

<iframe src="http://player.vimeo.com/video/216794684" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>

<p>Video by Institut national d'Audiovisuel (INA)
© INA 2016</p>
<hr />
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">#</a></h2>
<p>Much of my design practice involves computer scripting. In the previous chapter, I described two projects which relied heavily on computer scripting: <em>Ant Ballet</em> used an inverse-kinematics programme to drive its robot arm and spread pheromones; the <em>Godot Machine</em> used a programme which used computer vision to drive the movement of a ball.<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup> Chapter 3 introduces two projects where I used scripting to generate new films, <em>86400</em> and <em>24fps Psycho</em> from archival footage.<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup> I regard the act of writing code itself a design process: it requires a mode of thought that is procedural and concentrated; it is distinct from drawing, yet offers a similarly iterative process of refinement and improvement. This chapter will discuss the way in which designers, and in particular I, think <em>about code</em> and think <em>in code</em>. It begins with an introduction to principles of code writing, and a couple of precedents of diagramming code used by Italo Calvino and Alan Turing. It then presents two projects that form part of my research and design portfolio as part of this PhD, both of which use different principles and methodologies to frame a conversation about thinking about code.</p>
<p>My introduction to writing code (besides playing around with website creation in the early 2000s) came when I joined the Bartlett. I took a series of classes in the Interactive Architecture Lab which introduced me to two languages which I would then use for almost every project for the next few years: Processing and Arduino. Both languages are open-source, and were specifically written to enable artists and designers easy access to higher-level computer compiling languages and microcontrollers. Both also call their files ‘sketches’ – a tactical decision which both unites digital and analogue practices. The ‘sketch’ (rather than, say, file, script, document, or programme) contains clear implicit meaning: firstly, that code itself need not be daunting, and writing a prototypical piece of code should be as natural as drawing on paper. Secondly, the nomenclature aims to position writing code squarely into the early stages of design process, rather than being an extra thing to think about after the designing is done. Both languages offer exactly the promises Oosterhout described in his article on scripting, and these languages acted as a bridging tool for me to learn more advanced modes of programming.<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup></p>
<p>The form of integrated coding I experienced at the Bartlett seems to reflect the pedagogical practices I have witnessed first-hand at several other design schools over the past few years. Today, it is not uncommon to see parametric scripting, small electronic prototypes, applications and websites, or other various coded outcomes to student projects.<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup> As discussed in the introduction to this thesis, some teaching methods relating to particular aspects of scripting culture, namely parametricist design practice, are highly exclusionary, and rely on rote copying-and-pasting of code examples, or rely extensively on particular libraries and algorithms which are then unquestioningly repeated.<sup id="fnref:5"><a class="footnote-ref" href="#fn:5">5</a></sup> But other aspects of the computer-scripting world are relatively easy to inhabit as an autodidact; there are numerous tutorials, example files, libraries, forums, and so on online which can be used to extend pre-existing skills. My own entry to coding was via classes at the Bartlett, but once I knew the basics of a couple of languages, and knew where to look, and what to look for, extending my vocabulary and skillset was simply a matter of investing time. The approach I have to writing code is different to the traditional computer programmer, for whom the aim is often to write lean, efficient code (often for someone else) which performs to the highest standard; my intention is to use code and code-based-thought to investigate, to learn, or to express design intent.</p>
<h2 id="code-writing-and-code-thinking">Code writing and code thinking<a class="headerlink" href="#code-writing-and-code-thinking" title="Permanent link">#</a></h2>
<p>In most instances, before I write code, I use a number of diagrammatic techniques to work out what I want the code do. These could be in the form of state transition diagrams, pseudo-code, flow charts, or diagrams of code-based ‘mechanisms’, with labelled inputs, outputs and procedures.<sup id="fnref:6"><a class="footnote-ref" href="#fn:6">6</a></sup> These diagrams are functional sketches; they show a potential solution to some form of data processing. Like working sketches, they are often altered, improved upon, or superseded later in the design process. Their function is to enable me to mentally model what is going to be happening within the computer itself, to interrogate the logic and data-flows, and make improvements before the functions are obfuscated by interdependent lines of code written in a specific language.</p>
<p>Although arguably becoming more central to certain designers’ process, writing code does require modes of practice that are in themselves distinct from other design processes. Code is written in <em>code</em> <em>language</em>, and like spoken languages, code languages require learning, practice and often an attitudinal change with regards to the way in which things are done. Communities of people who write code are often tribe-line in their allegiance, and code languages tend to impose a mode of working that cause them to be particularly suited to certain ways of working. PHP, a scripting language used primarily on websites, for example, is notorious for quick development (although lamented by many computer programmers for being ‘dirty’);<sup id="fnref:7"><a class="footnote-ref" href="#fn:7">7</a></sup> Python and its associated communities pride themselves on the <em>Zen of Python</em> and use terms such as ‘pythonic’ to describe best practice when writing in the language.<sup id="fnref:8"><a class="footnote-ref" href="#fn:8">8</a></sup></p>
<p>The essential process that one enters when diagramming a computer programme is the mental modelling of the programme they are going to write. In essence, one must think <em>as</em> a computer; to understand how at each step the programme will store, retrieve and manipulate symbols, variables, and objects. To draw a linguistic analogy, we might think about the difference between <em>having</em> and <em>being</em> in English and Spanish. In English, we draw no grammatical distinction between the state of being implicit in human gender, age, activity, or state of sensing (e.g. warm/cold). However, in Spanish, each of these attributes is approached differently:</p>
<p><a name="table2-2"></a></p>
<table>
<thead>
<tr>
<th>English</th>
<th>Spanish</th>
<th>Spanish-English translation</th>
</tr>
</thead>
<tbody>
<tr>
<td>I am a man</td>
<td>Soy uno hermano</td>
<td>I am [in the long-term] a man</td>
</tr>
<tr>
<td>I am thirty years old</td>
<td>Tengo trienta años</td>
<td>I have thirty years</td>
</tr>
<tr>
<td>I am writing</td>
<td>Estoy escribiendo</td>
<td>I am [in this moment] writing</td>
</tr>
<tr>
<td>I am a designer</td>
<td>Soy un diseñador</td>
<td>I am [permanently] a designer</td>
</tr>
<tr>
<td>I am cold</td>
<td>Tengo frio</td>
<td>I have cold</td>
</tr>
<tr>
<td>I have a headache</td>
<td>Me duele la cabeza</td>
<td>It hurts my head</td>
</tr>
</tbody>
</table>
<figure>
  <figcaption>Table 2-2:  English / Spanish translations. Source: the author.</figcaption>
</figure>

<p>Whilst in both English and Spanish, the solution to being (or having) cold may be to turn the radiator up, the effect of the temperature rise is expressed differently; in English, it refers to a stat of being (I am cold) and in Spanish the state of being is expressed through a possessed attribute (<em>Tengo frio</em>: I have cold). The presence of temporary and permanent states of being (e.g. <em>estoy</em> / <em>soy</em>) make the quick articulation of a vocation, as opposed to a temporary task, a simple verbal substitution, whereas in English, this would be declared or implied through context. This is a similar nuance to differing modes of computer programming: the object-oriented programmer may create a class of objects that store a variable for coldness (the equivalent of <em>being</em> cold), whereas another programming language may create a series of objects where ‘coldness’ is a secondary, external variable (the equivalent of <em>having</em> cold). Another may invoke coldness as a state. The differences in the handling of virtual objects, the changing of states and attributes, and the order in which things are processed, all contribute to the utility of a language at certain tasks.</p>
<p>Much of the time in my own practice, I use flow-charts, draw data flows, and write pseudo-code to work out the effect I would like my programme to have. This then allows me to put ‘sample data’ through the virtual programme and see the effect. For example, here is a sample piece of pseudo-code for a thermostat, designed to run continuously:</p>
<pre><code>Turning the radiator ON for 5 SECONDS increases the temperature by 1º.
Leaving the radiator OFF for 30 SECONDS decreases the temperature by 1º.

Every 5 SECONDS, read TEMPERATURE:
- If TEMPERATURE less than TARGET, switch radiator ON.
- If TEMPERATURE larger than or equal to TARGET, switch radiator OFF.
</code></pre>
<p>I would then ‘run’ this code several times on paper, writing down different starting temperatures, and the results as I went along.<sup id="fnref:9"><a class="footnote-ref" href="#fn:9">9</a></sup> Once I was happy that the pseudo-code would deliver my desired results, I would then turn this into real code (the following to be run in Python):</p>
<pre><code>target = 18
currentTemp = getTemp()

if (currentTemp &lt; target):
  radiator = 1
else:
  radiator = 0
</code></pre>
<p>This process is not unusual for a programmer. Pseudo-code and flowcharts formed part of my first code lessons; the methodology is rife online, and anecdotally, every programmer I have spoken to about the subject uses some form of pseudo-coding if they are mounting a complex task.</p>
<p>Another well-known pseudo-coder was Italo Calvino. As discussed in this thesis’ introduction, Calvino was a member of the Oulipo and active experimenter with combinatorial forms of story-writing. Calvino described his early theories of combinatorial writing in his 1967 lecture <em>Cybernetics and Ghosts</em>.<sup id="fnref:10"><a class="footnote-ref" href="#fn:10">10</a></sup> Alluding the theory of DNA, which enables ‘the endless variety of living forms’ to be ‘reduced to a combination of certain finite quantities’, he turns the same logic to linguistics:<sup id="fnref:11"><a class="footnote-ref" href="#fn:11">11</a></sup></p>
<blockquote>
<p>Here again, it is information theory that imposes its patterns. The processes that appeared most resistant to a formulation on terms of number, to a quantitative description, are not translated into mathematical patterns.<sup id="fnref:12"><a class="footnote-ref" href="#fn:12">12</a></sup></p>
</blockquote>
<p>This is perhaps most exemplified with the story the <em>Burning of the Abominable House.</em><sup id="fnref:13"><a class="footnote-ref" href="#fn:13">13</a></sup> In writing this story, which was in itself based on combinatory principles, Calvino effectively <em>became</em> the computer, running his pseudo-code instructions line by line to create enough possible permutations to tell the story. His widow, Esther Calvino, recounts:</p>
<blockquote>
<p>There had been a somewhat vague request from IBM: how far was it possible to write a story using the computer? This was in 1973 in Paris when it wasn’t easy to gain access to data processing equipment. Undaunted, Calvino gave the project a Calvino gave the project a great deal of his time, carrying out all of the operations himself. The story was finally published in the Italian edition of <em>Playboy</em>. Calvino didn’t really feel this was a problem, although he had originally planned for it to be published in <em>Oulipo</em> as an example of <em>ars combinatoria</em> and a challenge to his own mathematical abilities.<sup id="fnref:14"><a class="footnote-ref" href="#fn:14">14</a></sup></p>
</blockquote>
<p>The expense of computers did not just create human-computers of Italian writers in the 1970s. Alan Turing, whose 1936 paper <em>On computable numbers, with an application to the Entscheidungsproblem</em> set forth the concept of the universal computing machine (the basis of every modern computer), and whose pioneering work developing early computers led to the cracking of the Enigma code (shortening WWII by several years), was himself unable to access a computer in 1951 whilst trying to write a programme that enabled a digital computer to play chess.<sup id="fnref:15"><a class="footnote-ref" href="#fn:15">15</a></sup> Turing was convinced that programming computers to solve complex problems such as playing chess would yield unexpected advances in other sectors:</p>
<blockquote>
<p>Research into the techniques of programming may in fact lead to quite important advances, and help in serious business and economics – perhaps, regrettably, even in the theory of war[^.]<sup id="fnref:16"><a class="footnote-ref" href="#fn:16">16</a></sup></p>
</blockquote>
<p>His first attempts at writing a chess programme were on paper, without computer, and he tested them by playing friends and secretaries – with Turing playing the role of computer himself, running through the algorithms for each move.<sup id="fnref:17"><a class="footnote-ref" href="#fn:17">17</a></sup> Turing used the concept of computers playing chess to argue the following principle:</p>
<blockquote>
<p>If one can explain quite unambiguously in English, with the aid of mathematical symbols if required, how a calculation is to be done, then it is always possible to programme any digital computer to do that calculation, provided the storage capacity is adequate.<sup id="fnref:18"><a class="footnote-ref" href="#fn:18">18</a></sup></p>
</blockquote>
<p>The idea of becoming a human computer to complete complex mathematical tasks seems absurd<sub>a</sub> today, when so much of our contemporary calculation is done via digital computers. It would seem even more absurd<sub>a</sub> if the person doing the computing is a famous author or one of the originators of computers themselves. But as both Nancy Katherine Hayles and David Allen Grier note, <em>human computers</em> (a job predominantly performed by women), were commonplace in the 1930s and 40s.<sup id="fnref:19"><a class="footnote-ref" href="#fn:19">19</a></sup> Carrying out large, often mundane calculations required that banks of women may work together on separate parts of the same task.<sup id="fnref:20"><a class="footnote-ref" href="#fn:20">20</a></sup> It is only in 1946, after the pioneering work of John von Nuemann and Alan Turing, that the term came to mean a non-human device which would calculate. The first OED citation of term ‘computer’ is in fact from 1613, and refers to a person who makes calculations. Grier claims that ‘the start of large-scale scientific research’ now ‘known informally as “big science”’ can be ‘traced to the [^human] computing offices of the eighteenth and nineteenth centuries.’<sup id="fnref:21"><a class="footnote-ref" href="#fn:21">21</a></sup></p>
<p>Today, however, digital computation is increasingly ubiquitous. ‘Cloud-based’ computing, ‘internet of things’, smartphones, and digital assistants such as Google Assistant and Amazon Alexa have all emerged in the past few years, and all bear the promise of extending the reach of computation.<sup id="fnref:22"><a class="footnote-ref" href="#fn:22">22</a></sup> The systems behind Google Assistant and Amazon Alexa in particular are marketed as making use of artificial intelligence and/or machine learning, two widely used terms that are now even commonly <span id="_Ref481774066" class="anchor"></span>found in tabloid news headlines.<sup id="fnref:23"><a class="footnote-ref" href="#fn:23">23</a></sup> Many large-scale consumer technology companies (Google, Apple, Facebook, Amazon, and more) are investing in artificial intelligence research;<sup id="fnref:24"><a class="footnote-ref" href="#fn:24">24</a></sup> machine learning is also at the heart of emerging self-piloting technologies such self-driving cars and delivery drones.<sup id="fnref:25"><a class="footnote-ref" href="#fn:25">25</a></sup> The recent boom in artificial intelligence is not the first; theoretical and practical research in the field has been carried out for at least fifty years. An earlier boom in research in the late 1970s and early 1980s was in part fuelled by researchers such as Minsky, and Schank and Abelson; the subject was so prevalent in the public consciousness that in 1984, the BBC commissioned philosopher John Searle to give their annual Reith Lectures on advances in computing, artificial intelligence, and their potential philosophical ramifications.<sup id="fnref:26"><a class="footnote-ref" href="#fn:26">26</a></sup></p>
<p>Searle is best known for his Chinese Room argument, which is a ‘refutation of strong artificial intelligence.’<sup id="fnref:27"><a class="footnote-ref" href="#fn:27">27</a></sup> Introduced in a paper entitled <em>Minds, Brains, and Programs</em> in 1980, the debate around the implications and validity of the argument still reins today.<sup id="fnref:28"><a class="footnote-ref" href="#fn:28">28</a></sup> The argument was originally posed as a response to researchers such as Schank and Abelson, who advocated that artificial intelligence programmes which could make use of linguistic scene-recognition would in some way be able to express understanding of scenes;<sup id="fnref:29"><a class="footnote-ref" href="#fn:29">29</a></sup> Searle advocates strongly against this viewpoint, arguing that computers will never be able to understand in the way that humans do, even if they can pass the Turing Test.<sup id="fnref:30"><a class="footnote-ref" href="#fn:30">30</a></sup> Searle summarises the argument concisely as follows:</p>
<blockquote>
<p>Imagine a native English speaker, let’s say a man, who knows no Chinese locked in a room full of boxes of Chinese symbols (a data base) together with a book of instructions for manipulating the symbols (the program). Imagine that people outside the room send in other Chinese symbols which, unknown to the person in the room, are questions in Chinese (the input). And imagine that by following the instructions in the program the man in the room is able to pass out Chinese symbols that are correct answers to the questions (the output). The program enables the person in the room to pass the Turing test for understanding Chinese, but he does not understand a word of Chinese.</p>
<p>The point of the argument is this: if the man in the room does not understand Chinese on the basis of implementing the appropriate program for understanding Chinese, then neither does any other digital computer solely on that basis because no computer, qua computer, has anything the man does not have.<sup id="fnref:31"><a class="footnote-ref" href="#fn:31">31</a></sup></p>
</blockquote>
<p>I was drawn to the Chinese Room argument due to its inherently analogue reasoning about the nature of artificial intelligence (the metaphor does not rely on a digital computer <em>per se</em>), and the implicit spatial and performative nature of the images elicited by Searle’s description. Through a diagram, purely described in language, Searle is able to make a rational, yet performative argument which challenges pre-existing notions about concepts such as mind and cognition.<sup id="fnref:32"><a class="footnote-ref" href="#fn:32">32</a></sup> <em>Nybble</em> is a project I developed which sought to make explicit the diagram Searle described through the participation of a bank of distinctly non-digital <em>human computers</em> blindly following instruction. It was performed in a digital festival in 2013. Three years later, <em>Scriptych</em> was a project which employed machine learning techniques to create a new mode of interaction with a computer database, using spatial metaphors to think about multi-dimensional data structures. It was performed in the entrance to the Opera Garnier in Paris, by dancers from the Opera – an area not traditionally used for the company’s performances. Both projects made use of dancers and were performances, and both did not make a traditional computer interface (i.e. screen) visible.</p>
<h2 id="nybble">Nybble<a class="headerlink" href="#nybble" title="Permanent link">#</a></h2>
<figure>
  <a name="figure2-13"></a>
  <img src="../images/figure2-13.jpg" width="100%" />
  <figcaption>Figure 2-13:  Nybble performance at the V&A Museum, September 2013. Photo by Danielle Willkens.</figcaption>
</figure>

<p>In June 2013 I was an exhibitor in the V&amp;A Museum’s Digital Design Weekend. This annual celebration, which I had first attended in 2009, coincided with the final weekend of London Design Festival, in which the V&amp;A were also participants. As such, it attracted large amounts of people interested in both digital culture and design. I was approached by the Digital Programmes Manager, Irini Papadimitriou, with whom I discussed creating a new piece of work to engage people with the concepts <em>surrounding</em> computation, rather than showcasing a new piece of computational artwork. Earlier that year, and completely independently, I had been a participant in a workshop run by an Arts and Humanities Research Council (AHRC) research consortium between the V&amp;A Museum and UCL’s Centre for Sustainable Heritage entitled Design with Heritage (DwH). The consortium’s aim was to investigate potential ways in which new and emerging technology could be used to enhance and augment exhibitions and artefacts within museums, and their working method was to gather industry professionals and independent practitioners together in intensive one-day workshops to rapidly identify and propose projects to investigate issues surrounding two topics: potential new methods in which existing and emerging technology could be used to enhance and augment exhibitions and artefacts within museums. I was fortunate to be awarded a small grant to investigate audience navigation in museums, which was used to produce the project that became <em>Nybble</em>.</p>
<p>At the time, the concept of artificial intelligence and machine learning was re-entering the public consciousness, and more media coverage was devoted to the subject. I had recently heard about Turing’s chess programme and began my engagement in issues surrounding machine learning and artificial intelligence.<sup id="fnref:33"><a class="footnote-ref" href="#fn:33">33</a></sup> I had also taught students computer programming and electronics via Arduino and Processing, and consistently found that students who were otherwise well-versed in using computers for advanced design purposes were often ill-able to describe the underlying principles of computing, or the lineage of technological and philosophical development which had led to the computers we use every day. It seemed that there was a widespread misconception about what a computer is and does; the focus in media representation of technology is often concerned with the form factor technology adopts (e.g. the specifications and aesthetics of a new gadget) rather than the wider ramifications of what it can do, and how this affects society. Turing’s influential papers on computation, and even his early experiments with chess programmes, and even Searle’s arguments about artificial intelligence, had been conducted without the use of a computer, yet the work of both continues to resonate today. I wanted to create a work Digital Design Weekend where computation was on display, but a digital computer was not.</p>
<p>I decided that an exhibition whereby a bank of humans could act out the inner workings of a computer, much like Calvino and Turing had done, would be an interesting way to present computation. The humans would form part of a huge computer, calculating something that only an audience would see; thus, the human computers inside the installation, would be acting out Searle’s Chinese Room argument. This would be a larger-scale version of the type of diagramming that I had previously attempted with the <em>Godot Machine</em> and <em>Ant Ballet</em>. The performance would also extend my work into the world of the Absurd, though through an inversion of the traditional Absurdist performance; if the absurd<sub>b</sub> is constructed through an ultimately doomed quest to find meaning in a universe devoid of it (in theatre, usually occurring through an initial establishment of normalcy followed by an entropic descent into meaninglessness), this piece would appear devoid of meaning for its performers and audience without a ‘key’, yet deliver an arbitrary message to those determined enough to persist and try to crack the code.</p>
<p>This structure was influenced by the arguably absurd<sub>b</sub> 1959 novel the <em>The</em> <em>Sirens of Titan</em> by Kurt Vonnegut.<sup id="fnref:34"><a class="footnote-ref" href="#fn:34">34</a></sup> The punch-line of the sci-fi story is that the whole of humanity’s development and progress – technology, languages, religions, and culture – have in fact been mysteriously influenced by forces from afar, in order that an elaborate and highly specific life-trajectory would be taken by one man, resulting in war on earth and mass destruction – all so that one small part of a broken machine on Mars would be replaced, enabling it to continue its journey across the universe to deliver a message. Finally, it is revealed that the message itself consists of a single point, meaning ‘Greetings’ in its native language.<sup id="fnref:35"><a class="footnote-ref" href="#fn:35">35</a></sup> Although smaller in scale, and notably devoid of conspiracy theory-paranoia, this project would ask audience and participants to decode something that tested the limits of their perception (and patience), yet itself conveyed a meaningless message, echoing the absurd<sub>b</sub> assertion that humanity is doomed to search for meaning in a universe devoid of it. Many of the concepts surrounding this idea were the result of conversations with colleagues in who shared a studio, co-tutors, my PhD supervisors, and my frequent collaborator, Abi Palmer.</p>
<h3 id="methodology">Methodology<a class="headerlink" href="#methodology" title="Permanent link">#</a></h3>
<p>Please note, for the sake of clarity, the project is presented here in thematic, rather than strictly chronological order. Much of the project development, as with most projects I work on, occurred concurrently as a result of the design process.</p>
<p>Early on in the design process, I began researching the mechanics of computation. The part of a computer that is most similar to the process that the Chinese Room argument describes is the Central Processing Unit (CPU). The primary function of computers is to interpret and manipulate symbols; and in order to process information in the correct order, a precise timing mechanism is required. This acts as a synchronisation and sequencing device that ensures information is processed in the right order.</p>
<p>The process of parsing characters from binary information held in a computer’s memory to symbols occurs every time a character is displayed on a computer screen. In essence, it is the most basic process a computer can do. At the simplest level, a sentence stored in computer memory is a string of binary bits – in the case of magnetic hard drive, these are magnetically charged particles that are either in an <em>on</em> or <em>off</em> state (represented as a <code>1</code> or a <code>0</code>).<sup id="fnref:36"><a class="footnote-ref" href="#fn:36">36</a></sup> In the case of the 1972 American Standard Code for Information Interchange (ASCII), a common text-storage system, seven bits are used to represent any one of 128 characters, encompassing capital and lowercase letters, numbers, punctuation symbols and computer teletype instructions (see Figure 2-20). For example, the uppercase letter <em>H</em> is represented by <code>0001001</code>. A sentence could be represented by simply stringing multiple strings together (so <em>Hello</em> would be <code>0001001 1010011 0011011 0011011 1111011</code> – spaces added for clarification). I decided that parsing information, character by character, would be an appropriate thing for the performance to diagram. I also decided that it would be interesting to employ a bank of human computers to parse information through dance, standing in the V&amp;A Museum. This would fit with the principles that I sought to investigate through the DwH programme, as well as display a computational principle for the Digital Design Weekend.</p>
<p>I then needed a means of making the performance as visible as possible – and changing the way that people moved around the space. The mode of operation would have to employ some sort of binary state in order to unambiguously reflect digital storage, conveying information in a highly visible way, but still require the audience to decode it. Like a CPU, whose internal clock is invisible to a computer user, the internal instructions fed to each dancer would be hidden from the public, yet synchronise the actions of the dancers. I decided to use four dancers because that would simultaneously allow for a multitude of characters to be conveyed via a lookup table, and provide a means of making the performance large, and visible to the public.</p>
<p>I commissioned costume designer Magdalena Gustafson to work on costumes that would extend the physical reach of the designers. Being interested in the binary nature of the movements the dancers would be able to move to, we devised a system whereby each dancer would wear two large ‘sails’, around 3.4m high, which would attach to their back via a pair of masts affixed to a backpack (the backpacks themselves were constructed from modified baby transporters; the poles were modified lightweight fishing rods). The ‘boom’ of each sail would be held in the dancers’ hand, and extend their reach by approximately 2.2m in each direction. This would allow the dancers to use a semaphore-like mode of signalling to move into one of four positions: both sails down; both sails up; left-hand sail down/right-hand sail up; right-hand sail down/left-hand sail up; and both sails down. These positions were called 0-3 (in computing, zero is usually the first number that is used for counting).</p>
<hr />
<h2 id="nybble-dance-instructions-sheet-21-september-2013">Nybble dance instructions sheet: 21 September 2013<a class="headerlink" href="#nybble-dance-instructions-sheet-21-september-2013" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th>Move No.</th>
<th>Dancer A</th>
<th></th>
<th>Dancer B</th>
<th></th>
<th>Dancer C</th>
<th></th>
<th>Dancer D</th>
<th></th>
<th>Symbol represented</th>
<th>ASCII Table equivalent</th>
<th>Symbol</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Direction</td>
<td>Position</td>
<td>Direction</td>
<td>Position</td>
<td>Direction</td>
<td>Position</td>
<td>Direction</td>
<td>Position</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>BACK</td>
<td>0</td>
<td>FRONT</td>
<td>0</td>
<td>BACK</td>
<td>0</td>
<td>BACK</td>
<td>0</td>
<td>4</td>
<td><code>D</code></td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>D</td>
<td>1</td>
<td><code>a</code></td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>D</td>
<td>14</td>
<td><code>n</code></td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>FRONT</td>
<td>D</td>
<td>3</td>
<td><code>c</code></td>
</tr>
<tr>
<td>4</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>D</td>
<td>5</td>
<td><code>e</code></td>
</tr>
<tr>
<td>5</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>B</td>
<td>15</td>
<td><code>_</code></td>
</tr>
<tr>
<td>6</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>D</td>
<td>6</td>
<td><code>f</code></td>
</tr>
<tr>
<td>7</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>D</td>
<td>9</td>
<td><code>i</code></td>
</tr>
<tr>
<td>8</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>BACK</td>
<td>C</td>
<td>2</td>
<td><code>r</code></td>
</tr>
<tr>
<td>9</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>C</td>
<td>3</td>
<td><code>s</code></td>
</tr>
<tr>
<td>10</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>BACK</td>
<td>C</td>
<td>4</td>
<td><code>t</code></td>
</tr>
<tr>
<td>11</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>C</td>
<td>15</td>
<td><code>.</code></td>
</tr>
<tr>
<td>12</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>B</td>
<td>15</td>
<td><code>_</code></td>
</tr>
<tr>
<td>13</td>
<td>1</td>
<td>BACK</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>BACK</td>
<td>1</td>
<td>BACK</td>
<td>B</td>
<td>4</td>
<td><code>T</code></td>
</tr>
<tr>
<td>14</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>D</td>
<td>8</td>
<td><code>h</code></td>
</tr>
<tr>
<td>15</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>D</td>
<td>9</td>
<td><code>i</code></td>
</tr>
<tr>
<td>16</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>D</td>
<td>14</td>
<td><code>n</code></td>
</tr>
<tr>
<td>17</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>FRONT</td>
<td>D</td>
<td>11</td>
<td><code>k</code></td>
</tr>
<tr>
<td>18</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>B</td>
<td>15</td>
<td><code>_</code></td>
</tr>
<tr>
<td>19</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>BACK</td>
<td>D</td>
<td>12</td>
<td><code>l</code></td>
</tr>
<tr>
<td>20</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>D</td>
<td>1</td>
<td><code>a</code></td>
</tr>
<tr>
<td>21</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>BACK</td>
<td>C</td>
<td>4</td>
<td><code>t</code></td>
</tr>
<tr>
<td>22</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>D</td>
<td>5</td>
<td><code>e</code></td>
</tr>
<tr>
<td>23</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>BACK</td>
<td>C</td>
<td>2</td>
<td><code>r</code></td>
</tr>
<tr>
<td>24</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>C</td>
<td>15</td>
<td><code>.</code></td>
</tr>
<tr>
<td>25</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>B</td>
<td>15</td>
<td><code>_</code></td>
</tr>
<tr>
<td>26</td>
<td>0</td>
<td>FRONT</td>
<td>0</td>
<td>BACK</td>
<td>0</td>
<td>BACK</td>
<td>0</td>
<td>FRONT</td>
<td>A</td>
<td>9</td>
<td><code>I</code></td>
</tr>
<tr>
<td>27</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>BACK</td>
<td>C</td>
<td>4</td>
<td><code>t</code></td>
</tr>
<tr>
<td>28</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>D</td>
<td>0</td>
<td><code>'</code></td>
</tr>
<tr>
<td>29</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>C</td>
<td>3</td>
<td><code>s</code></td>
</tr>
<tr>
<td>30</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>B</td>
<td>15</td>
<td><code>_</code></td>
</tr>
<tr>
<td>31</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>BACK</td>
<td>C</td>
<td>4</td>
<td><code>t</code></td>
</tr>
<tr>
<td>32</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>D</td>
<td>8</td>
<td><code>h</code></td>
</tr>
<tr>
<td>33</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>D</td>
<td>5</td>
<td><code>e</code></td>
</tr>
<tr>
<td>34</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>B</td>
<td>15</td>
<td><code>_</code></td>
</tr>
<tr>
<td>35</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>D</td>
<td>14</td>
<td><code>n</code></td>
</tr>
<tr>
<td>36</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>D</td>
<td>1</td>
<td><code>a</code></td>
</tr>
<tr>
<td>37</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>BACK</td>
<td>C</td>
<td>4</td>
<td><code>t</code></td>
</tr>
<tr>
<td>38</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>FRONT</td>
<td>C</td>
<td>5</td>
<td><code>u</code></td>
</tr>
<tr>
<td>39</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>BACK</td>
<td>C</td>
<td>2</td>
<td><code>r</code></td>
</tr>
<tr>
<td>40</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>D</td>
<td>1</td>
<td><code>a</code></td>
</tr>
<tr>
<td>41</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>D</td>
<td>12</td>
<td><code>l</code></td>
</tr>
<tr>
<td>42</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>1</td>
<td>FRONT</td>
<td>B</td>
<td>15</td>
<td><code>_</code></td>
</tr>
<tr>
<td>43</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>FRONT</td>
<td>D</td>
<td>15</td>
<td><code>o</code></td>
</tr>
<tr>
<td>44</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>BACK</td>
<td>C</td>
<td>2</td>
<td><code>r</code></td>
</tr>
<tr>
<td>45</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>BACK</td>
<td>D</td>
<td>4</td>
<td><code>d</code></td>
</tr>
<tr>
<td>46</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>3</td>
<td>BACK</td>
<td>3</td>
<td>FRONT</td>
<td>D</td>
<td>5</td>
<td><code>e</code></td>
</tr>
<tr>
<td>47</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>BACK</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>BACK</td>
<td>C</td>
<td>2</td>
<td><code>r</code></td>
</tr>
<tr>
<td>48</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>2</td>
<td>FRONT</td>
<td>C</td>
<td>15</td>
<td><code>.</code></td>
</tr>
</tbody>
</table>
<h2 id="nybble-cheat-sheet-21-september-2013">Nybble cheat sheet: 21 September 2013<a class="headerlink" href="#nybble-cheat-sheet-21-september-2013" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th>Row</th>
<th>Direction A</th>
<th>Direction B</th>
<th>Direction C</th>
<th>Direction D</th>
<th>Position 0</th>
<th>Position 1</th>
<th>Position 2</th>
<th>Position 3</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>BACK</td>
<td>BACK</td>
<td>BACK</td>
<td>BACK</td>
<td><code>@</code></td>
<td><code>P</code></td>
<td>`</td>
<td><code>p</code></td>
</tr>
<tr>
<td>1</td>
<td>BACK</td>
<td>BACK</td>
<td>BACK</td>
<td>FRONT</td>
<td><code>A</code></td>
<td><code>Q</code></td>
<td><code>a</code></td>
<td><code>q</code></td>
</tr>
<tr>
<td>2</td>
<td>BACK</td>
<td>BACK</td>
<td>FRONT</td>
<td>BACK</td>
<td><code>B</code></td>
<td><code>R</code></td>
<td><code>b</code></td>
<td><code>r</code></td>
</tr>
<tr>
<td>3</td>
<td>BACK</td>
<td>BACK</td>
<td>FRONT</td>
<td>FRONT</td>
<td><code>C</code></td>
<td><code>S</code></td>
<td><code>c</code></td>
<td><code>s</code></td>
</tr>
<tr>
<td>4</td>
<td>BACK</td>
<td>FRONT</td>
<td>BACK</td>
<td>BACK</td>
<td><code>D</code></td>
<td><code>T</code></td>
<td><code>d</code></td>
<td><code>t</code></td>
</tr>
<tr>
<td>6</td>
<td>BACK</td>
<td>FRONT</td>
<td>FRONT</td>
<td>BACK</td>
<td><code>F</code></td>
<td><code>V</code></td>
<td><code>f</code></td>
<td><code>v</code></td>
</tr>
<tr>
<td>7</td>
<td>BACK</td>
<td>FRONT</td>
<td>FRONT</td>
<td>FRONT</td>
<td><code>G</code></td>
<td><code>W</code></td>
<td><code>g</code></td>
<td><code>w</code></td>
</tr>
<tr>
<td>8</td>
<td>FRONT</td>
<td>BACK</td>
<td>BACK</td>
<td>BACK</td>
<td><code>H</code></td>
<td><code>X</code></td>
<td><code>h</code></td>
<td><code>x</code></td>
</tr>
<tr>
<td>9</td>
<td>FRONT</td>
<td>BACK</td>
<td>BACK</td>
<td>FRONT</td>
<td><code>I</code></td>
<td><code>Y</code></td>
<td><code>i</code></td>
<td><code>y</code></td>
</tr>
<tr>
<td>10</td>
<td>FRONT</td>
<td>BACK</td>
<td>FRONT</td>
<td>BACK</td>
<td><code>J</code></td>
<td><code>Z</code></td>
<td><code>j</code></td>
<td><code>z</code></td>
</tr>
<tr>
<td>11</td>
<td>FRONT</td>
<td>BACK</td>
<td>FRONT</td>
<td>FRONT</td>
<td><code>K</code></td>
<td><code>[</code></td>
<td><code>k</code></td>
<td><code>{</code></td>
</tr>
<tr>
<td>12</td>
<td>FRONT</td>
<td>FRONT</td>
<td>BACK</td>
<td>BACK</td>
<td><code>L</code></td>
<td><code>\</code></td>
<td><code>l</code></td>
<td><code></code></td>
</tr>
<tr>
<td>13</td>
<td>FRONT</td>
<td>FRONT</td>
<td>BACK</td>
<td>FRONT</td>
<td><code>M</code></td>
<td><code>]</code></td>
<td><code>m</code></td>
<td><code>}</code></td>
</tr>
<tr>
<td>14</td>
<td>FRONT</td>
<td>FRONT</td>
<td>FRONT</td>
<td>BACK</td>
<td><code>N</code></td>
<td><code>^</code></td>
<td><code>n</code></td>
<td><code>~</code></td>
</tr>
<tr>
<td>15</td>
<td>FRONT</td>
<td>FRONT</td>
<td>FRONT</td>
<td>FRONT</td>
<td><code>O</code></td>
<td><code>_</code></td>
<td><code>o</code></td>
<td><code>.</code></td>
</tr>
</tbody>
</table>
<p>Dancers’ individual direction represents row)<br />
All dancers in position (represents column)</p>
<figure>
  <a name="figure2-12"></a>
  <figcaption>Figure 2-12: Dance instruction sheet, and ‘code cheat sheet’ from Saturday 21<sup>st</sup> September 2013.</figcaption>
</figure>

<hr />
<figure>
  <a name="figure2-14"></a>
  <img src="../images/figure2-14.jpg" width="100%" />
  <figcaption>Figure 2-14:  Nybble performance at the V&A Museum, September 2013. Photo by Danielle Willkens.</figcaption>
</figure>

<hr />
<figure>
  <a name="figure2-15"></a>
  <img src="../images/figure2-15.jpg" width="100%" />
  <figcaption>Figure 2-15:  Nybble performance at the V&A Museum, September 2013. Photo by Danielle Willkens.</figcaption>
</figure>

<hr />
<figure>
  <a name="figure2-16"></a>
  <img src="../images/figure2-16.1.jpg" width="100%" />
  <img src="../images/figure2-16.2.jpg" width="100%" />
  <img src="../images/figure2-16.3.jpg" width="100%" />
  <img src="../images/figure2-16.4.jpg" width="100%" />
  <figcaption>Figure 2-16:  Four photos of Nybble performance at the V&A Museum, September 2013, by Danielle Willkens.</figcaption>
</figure>

<hr />
<figure>
  <a name="figure2-17"></a>
  <img src="../images/figure2-17.jpg" width="100%" />
  <figcaption>Figure 2-17:  Nybble performance at the V&A Museum, September 2013. Photo by the author.</figcaption>
</figure>

<hr />
<figure>
  <a name="figure2-18"></a>
  <img src="../images/figure2-18.jpg" width="100%" />
  <figcaption>Figure 2-18:  Nybble performance at the V&A Museum, 21-23 September 2013. Photo by Danielle Willkens.</figcaption>
</figure>

<hr />
<figure>
  <a name="figure2-19"></a>
  <img src="../images/figure2-19.png" width="100%" />
  <figcaption>Figure 2-19:  Code guide for Nybble performance, featuring sixty four possible symbols. By the author.</figcaption>
</figure>

<hr />
<figure>
  <a name="figure2-20"></a>
  <img src="../images/figure2-20.png" width="100%" />
  <figcaption>Figure 2-20:  USASCII Code Chart. ‘ASCII - Wikipedia.’ Accessed 11 May 2017. [https:// en.wikipedia.org/wiki/ASCII#/media/File:US-ASCII_code_chart.png](https:// en.wikipedia.org/wiki/ASCII#/media/File:US-ASCII_code_chart.png). US-ASCII Code Chart. Scanner copied from the material delivered with TermiNet 300 impact type printer with Keyboard, February 1972, General Electric Data communication Product Dept., Waynesboro VA</figcaption>
</figure>

<hr />
<figure>
  <a name="figure2-21"></a>
  <img src="../images/figure2-21.jpg" width="100%" />
  <figcaption>Figure 2-21:  Nybble performance at the V&A Museum, 21-23 September 2013. Photo by Daneille Willkens.</figcaption>
</figure>

<hr />
<p>In order to safely exhibit dancers who were wearing large bodily extensions, I worked with the curator to find a site suitable for the dance. Internal spaces within the institution are highly regulates spaces due to the number of valuable artefacts. We decided to place the dancers in the John Madjeski Garden: it had high visibility, and an almost stage-like setup around a pond, and the people there tended to be taking a ‘break’ from the main exhibition spaces to eat and drink (in short, a captive audience). The costume designer and I decided to use bold, electric blue and light green colours for the sails, both to compliment the iconic Dale Chihuly chandelier in the V&amp;A’s main entrance (the first thing most people see on entering the museum), and to stand out against the vivid orange of the bricks within the courtyard. The sails would have a clear directionality; if the dancer was facing towards the audience, they would only see blue, whereas rear-facing dancers would reveal a flash of bright green, aiding the interpretation of the code the dancers would be parsing.</p>
<p>The development of the code was an area that required solving fairly early on in the process. Abi, who would be the controller, and I discussed multiple possibilities, but our priority was the ease and accuracy of conveying a message to each of the dancers on the day, telling them what to do. We decided to use a walkie-talkie system to communicate with the dancers; this would enable the transmission of instructions without the public’s knowledge. We decided on a message to parse: a popular misquote from Samuel Beckett’s <em>Waiting for Godot</em> which accurately encapsulated what the dancers would actually be doing:</p>
<blockquote>
<p>Dance first. Think later. It’s the natural order.<sup id="fnref:37"><a class="footnote-ref" href="#fn:37">37</a></sup></p>
</blockquote>
<p>The use of a misquote was a conscious decision; the message was sufficiently meaningless, and devoid of context, that the entire dance could be rendered absurd<sub>a</sub> and absurd<sub>b</sub>. In order to ensure that the dancers accurately represented Searle’s argument, none were told the message until after all of the performances were over.</p>
<p>The next decision was about the code-table that the dancers would be effectively parsing. Four dancers moving to four different positions, and each facing one of two directions simultaneously could represent 512 potential symbols in an ASCII-style table. This was far too much detail – if the overall dance were to portray a message in English, it would require only capital or lowercase letters (52), plus a small array of punctuation symbols. In order that the audience stood some chance of being able to interpret the dance, I reduced the potential symbols the dancers could represent to 64. The dancers would all move simultaneously to one of the four listed positions above (representing the column in Figure 2-19) and the direction that the dancers faced would indicate the row. With this table configuration, the word <em>Hello</em> would consist of five moves. This was, technically, a sub-optimal mode of information storage – I could have used a lookup table which utilised each of the dancers’ four positions in two directions separately, giving 4096 symbols, but had no real need for this level of complexity. It also aided the process of telling the dancers what to do: they were universally told to move to the same ‘position’, but the direction they faced was an individual instruction.</p>
<p>In order to enable the creation of a set of instructions for the controller to read, which would contain all 196 dancer positions to convey the 49 characters in the message, along with the dancers’ names, I built a simple programme in a Microsoft Excel spreadsheet that would provide detailed instructions, character-by-character, for each of the dancers. The task may have been easier to carry out in a lower-level programming language, using a simple three-dimensional array, but I enjoyed the absurd<sub>a</sub> nature of using a spreadsheet to determine an artistic performance. I believe that the spreadsheet is a largely unseen influential force in artistic and architectural practice; much of the work of producing a performance such as <em>Nybble</em> is balanced through the spreadsheets which carry the financial budgets and task and time allocation. The impact of the Excel spreadsheet is perhaps one of the most mundane, yet influential, means by which numerous decisions are really made within both art and design. I also took pleasure in using mundane, office-related software to drive an artwork in a festival which is supposed to celebrate the radical newness of digital design.</p>
<p>The dance sequencing itself was devised partly through dance workshops (which will be discussed later), but mostly through the assessment of how CPUs handle information stored in memory. When retrieving information from Random Access Memory (RAM), 50% of the memory is used as address; that is, merely to <em>locate</em> the information the computer is to process, whilst the other 50% is used to store the information itself. This implied that around half of the time spent processing information is spent locating it, or preparing to process. The dance we would carry out would follow a similar formula; the dancers would spend half of their time receiving instructions, and half of the time ‘processing’ them. Of course, this would happen at an order of magnitude slower than a digital computer; parsing the 49 characters of the message we conveyed would take approximately 25 minutes. Each symbol being represented would be one move; a position for all of the dancers to collectively move to, and a direction for each one to face individually. The instructions, and synchronisation cues, would be delivered by the controller (played by Abi) who would speak into a walkie-talkie over the top of a synchronisation soundtrack.</p>
<h3 id="soundtrack">Soundtrack<a class="headerlink" href="#soundtrack" title="Permanent link">#</a></h3>
<p>As a CPU is dependent on its internal clock for synchronisation, the dancers in <em>Nybble</em> would use a hidden timing mechanism to stay synchronised. This would enable the bank of dancers, who would be positioned several metres apart, to receive instructions and coordinate their moves to match each other. The dancers would each wear a walkie-talkie radio with an earpiece, tuned to the same frequency. At the other end, the controller would transmit a timing signal and instructions to the dancers. This meant that the signals of the soundtrack would have to be easily intelligible via a low-resolution radio bandwidth. In addition, I wanted the soundtrack to have the audio aesthetic of the 1940-50s Britain which Alan Turing had inhabited, and echoes of the Shipping Forecast. This last influence was of particular importance as I have been an avid listener of the late night Shipping Forecast on BBC Radio 4 for years, enjoying its soothing and rhythmic mode of delivering semi-coded information, yet never really known what the numbers meant – a similar state to how I imagined the audience would receive this work.</p>
<p>The audio track would have to be distinctive, carrying both spoken and audio cues, so that if a radio ceased functioning, or the whole message did not transmit, the dancers could still quickly assess where they were in the sequence. Every second, a computer-generated austere male voice would announce a count, between two and eight, with the ‘one’ count being replaced by the instruction to ‘hold’ or ‘go’ – that is, the dancers should adopt their pause positions or move. Between count, a sine wave beep (much like the ‘pips’ on BBC Radio 4) would play to establish continuity, and at the final count of the bar, a double-count would play:</p>
<pre><code class="language-shell">Hold beep 2 beep 3 beep 4 beep 5 beep 6 beep 7 beep 8 beep, beep
Go!  beep 2 beep 3 beep 4 beep 5 beep 6 beep 7 beep 8 beep, beep
</code></pre>
<p>In the time when dancers were transitioning between positions, they would be given an instruction as to how to move (e.g. ‘fire’, ‘underwater’). These instructions were the result of three workshops in which Abi worked with the dancers and established the particularities of giving live instructions for them to work with. The dancers needed something to focus on whilst moving into position, and the variation that this improvisation provided in also served to provide visual interest for the audience. The synchronisation track and instructions were delivered via walkie-talkie; the synchronisation track, which was looped, was played on repeat through an iPod, the earpiece of which was taped to the microphone of the controller’s headset. This meant that the controller could speak over the soundtrack, and the low-resolution radios (designed for use in loud environments such as nightclubs) would transmit the loudest noise. Each dancer wore a walkie-talkie, clipped to their rucksack, and a one-way earpiece.</p>
<p>The installation’s name was chosen close to the work’s performance, and was the result of a conversation with artist and (at the time) co-tutor in the Interactive Architecture Lab, Ruairi Glynn. The name emerged through thinking about the quantity of information each dancer would convey; their four potential positions would be analogous to half a byte (eight ‘bits’) of information. Within computing, half a byte is known as a <em>nybble</em> (word-play on the similarity of <em>byte</em> to <em>bite</em>). The concision of the word, along with its implicit humility, summarised the ethos of the performance.</p>
<h3 id="dancers">Dancers<a class="headerlink" href="#dancers" title="Permanent link">#</a></h3>
<p>This project represents the first time I had worked with dancers, and my expertise in this field was minimal. My friend Andrea Mongenie, who had been a casting agent for the London Olympics opening ceremony, offered a wealth of advice. In order to attract attention to the project, I created a trailer film from promotional footage of IBM computational systems, taken from the Prelinger Archives.<sup id="fnref:38"><a class="footnote-ref" href="#fn:38">38</a></sup> The teaser was designed to illicit interest from potential dancers. It combined the following dialogue with images of a computer parsing text:</p>
<blockquote>
<p>The computer is then given the problem in the form of numbers or instructions pertinent to arithmetic. It is the arithmetic logical task that the computer is organised to do. Once instructed, it can do as much arithmetic in a minute as a man in a lifetime.
A man in a lifetime…the lifetime of all mankind is but a brief moment in the long history of this earth of ours. And only yesterday in the history of mankind has man made any significant advance in his control over his earthly environment.<sup id="fnref:39"><a class="footnote-ref" href="#fn:39">39</a></sup></p>
</blockquote>
<p>I placed an advert on industry-standard casting websites, and held a casting session at a dance studio within UCL Union. We used a gender-blind system to track and rank candidates based on their experience, ability to receive and interpret instruction, hold a pose, and other relevant measures.<sup id="fnref:40"><a class="footnote-ref" href="#fn:40">40</a></sup> Once we had chosen the performers, Abi held three rehearsal sessions where the dancers were gradually introduced to the ideas contained within the performance, getting used to the bodily extensions that they would later be dancing with. In the third session, held in UCL’s Main Quad, the dancers were shown, and given the opportunity to test movement in a prototype costume made by Magdalena.</p>
<p>The performances were recorded and can be found in this thesis’ accompanying media.</p>
<h2 id="scriptych">Scriptych<a class="headerlink" href="#scriptych" title="Permanent link">#</a></h2>
<p>Nearly years after <em>Nybble</em>, during my residency at the Palais de Tokyo, I completed a second dance-based project, this time working in collaboration with the Opera de Paris (an arrangement that had been made by Ange Leccia, the founder and director of the Pavillon residency). The six artists who were resident would each team up with a choreographer and dancers from the Opera in order to create a performance or intervention in the publically-accessible areas of the Opera Garnier building for two evenings. We began working on this project as soon as we joined the Pavillon programme. I had known about this collaboration when I had been offered the position at the Palais de Tokyo some months earlier, so had begun to work on technical aspects of the project in advance.</p>
<figure>
  <a name="figure2-22"></a>
  <img src="../images/figure2-22.jpg" width="100%" />
  <figcaption>Figure 2-22:  Scriptych performance at Opera Garnier in June 2016. Photo by Justine Emard.</figcaption>
</figure>

<p>In the intervening time between <em>Nybble</em> and <em>Scriptych</em>, advances in artificial intelligence and machine learning had been widely publicised through media, and truly entered the public discourse. One of the major advances that had come to my awareness was the use of high-dimensional vector spaces to transform complex problems, such as translating languages and recognising the content of images, into spatial problems – albeit spaces consisting of potentially hundreds of vectors. This means that, much as Searle argued over thirty years before, computers were able to solve problems with the appearance of understanding their context and meaning, when really they were working through mathematical means. The application of machine learning to solve problems that we might categorise as about meaning (e.g. image-recognition, linguistic analysis) has been explored extensively by teams at Google.<sup id="fnref:41"><a class="footnote-ref" href="#fn:41">41</a></sup></p>
<p>With words, for example, the Word2vec algorithm (developed by Tomas Mikolov et al at Google) requires a huge dataset of words (the training dataset used is often the first 5 billion words from Wikipedia), which are then parsed into sentence form.<sup id="fnref:42"><a class="footnote-ref" href="#fn:42">42</a></sup> The algorithm tokenises all words, and sequentially calculates the word in question’s frequency, and its average distance from all other words. From this it can create a unique multi-dimensional vector for that word, consisting of several hundred dimensions. Through these complex word-vectors, implicit relationships between words emerge, allowing for a form of word-algebra to take place. The famous example that is regularly used to explain Word2vec is that the words for:</p>
<blockquote>
<p><code>(king – man) + woman = queen</code></p>
</blockquote>
<p>Similarly, the relationship between Athens and Greece is similar to Oslo and Norway, whilst common adjective-to-adverb relationships (e.g. apparent and apparently, rapid and rapidly) can be explained mathematically.<sup id="fnref:43"><a class="footnote-ref" href="#fn:43">43</a></sup> These word-relationships are common across languages, so that the vector for a word in English is likely to be similar to its equivalent in Spanish. As the MIT Technology Review reported in December 2014, a similar technique was tested with images.<sup id="fnref:44"><a class="footnote-ref" href="#fn:44">44</a></sup></p>
<p>The reduction of language to a series of mathematical vectors which would only be legible to computers reminded me of the frequent communication breakdowns in plays of the Theatre of the Absurd. As with Searle’s Chinese Room argument in <em>Nybble</em>, I wanted to carry out a new project to explore ideas surrounding this type of machine learning. I also wanted to carry out a performance in a way that experimented with a different approach to <em>Nybble</em>, developing a project that would contain scripting and rapid, iterative design as an integral part of the process in order to explore different modes of producing dance performances. However, I wished to retain the use of explicit constraints placed on performers.</p>
<p>In November 2015, the six artists in the Pavillon residency began having meetings with the Paris Opera in which artists would be paired with choreographers in order to produce an evening of performance in the Opera Garnier. From early on in the process, I knew that vector space would be intrinsic to the methodology of producing this work. In the period preceding the Opera project, I had spent two months in Chicago (where my partner had started a new job) before moving to Paris. Both of these transitions had made me question my own ability to communicate; I had become intrigued by the syntactic differences between British English and American English, which meant that I often struggled to, for example, order coffee – and in Paris, my grasp of French was pitiful. My own inability to communicate effectively became fuel and inspiration for <em>Scriptych</em>.</p>
<p>The first meeting with Opera was on the 19<sup>th</sup> of November. During this round-table discussion, I had expressed my desire to work with vector-space, and integrate some means of converting dancers’ movements into vectors, and consequently into words. Numerous dances contain ‘spatial conversations,’ whereby the movements of two or more dancers react to each other (or an audience), but this project would aim to take this idea a stage further: technologically mediated spatial conversations. The initial idea was primitive, and based solely around this principle; it was only when the true conversation and collaboration started between myself and the choreographer Simon Valastro that the performance evolved rapidly. This project was developed concurrently with other projects contained in this thesis – <em>Network / Intersect</em> and <em>24fps Psycho</em>, and its working title, until approximately one month before the performance took place, was, rather unoriginally, <em>Vector Space Translation</em>.</p>
<figure>
  <a name="figure2-23"></a>
  <img src="../images/figure2-23.png" width="100%">
  <figcaption>Figure 2-23:  Scriptych performance at Opera Garnier in June 2016. Photos by Justine Emard.</figcaption>
</figure>

<p>The group of artists visited the site of the performance, Opera de Garnier, both as building visitors, and to see several shows which featured the dancers and choreographers we would be working with. The building is much-discussed in architectural literature, and on one particularly memorable occasion we were taken on a long tour of the myriad hidden spaces which enable the building to operate: pulleys and levers under the stage, rehearsal studios, tunnels, and so on. The building appeared to be a large machine inhabited by its dedicated staff. I was particularly taken by the grand staircase which is encountered immediately after entering the building from the main entrance: Forty writes of the building’s architect Charles Garnier’s belief that the stair ‘is one of the most important arrangements (<em>dispositions</em>) in theatres because it is indispensable to the ease of arranging the exits (<em>dégagements</em>) and the circulation, but more because it produces an artistic motif.’<sup id="fnref:45"><a class="footnote-ref" href="#fn:45">45</a></sup> Upon entering the theatre, I was struck by the staircase’s inherent theatricality; visitors were transformed into performers, elevated for all to see. The same was true of the balconies surrounding the staircase: spaces of high visibility, and also granting vistas to visitors. Opera Garnier is a building for socialising as much as seeing performances, and the staircase signals the beginning of a journey into theatricality. The building seemed to epitomise Gage’s statement that ‘Architecture is differentiated from building in that it is deemed to induce sensations of delight or wonder in its observers.’<sup id="fnref:46"><a class="footnote-ref" href="#fn:46">46</a></sup></p>
<h3 id="design-process">Design process<a class="headerlink" href="#design-process" title="Permanent link">#</a></h3>
<p>My initial idea for the project revolved around translating dancers’ movement into vector-spatial data, converting this to words, and displaying some sort of ‘conversation’ between a pair of dancers. In order to prove this concept, I had to develop two things – an interface to interpret movement, and data to be interpreted. Over the period of January – March 2016, I experimented with Gensim, a Python-based framework for modelling with large datasets (which included Word2vec functions).<sup id="fnref:47"><a class="footnote-ref" href="#fn:47">47</a></sup> Over this period, I acquired multiple large-scale datasets (approximately 45Gb) and trained models with the intention of learning fundamentals of the principles to see the differing effects of various datasets and vector sizes, using online tutorials and guides.<sup id="fnref:48"><a class="footnote-ref" href="#fn:48">48</a></sup> My initial experiments yielded results that were of some use: it appeared that an optimum number of vectors was 300 per word, and ‘cleaner’ datasets yielded less erroneous results. However, each word library that I was creating was around 300,000-600,000 words, with words reflective of the miscellaneous sources: although the libraries contained many recognisable words and word-derivatives, they also contained highly technical words (e.g. <code>obgyn</code>, <code>vioxx</code>, <code>bextra</code>), online abbreviations (e.g. <code>wts</code>, <code>fccjobs</code>, <code>niccr</code>), mis-spelled or conjoined words (e.g. <code>constructech</code>, <code>advertisementbut</code>, <code>supportfootnotes</code>) and some which were meaningless and bizarre (e.g. <code>shinnecocks</code>, <code>athanassios</code>, <code>imclone</code>, <code>limbaughricans</code>, <code>ogio</code>).</p>
<p>The Gensim Word2vec Python library that I was using enabled several querying mechanisms: words could be directly queried (e.g. searching for the word <code>king</code> and returning its vector), or closest-match vectors could be calculated (e.g. searching for a particular vector and returning the closest word in the array of words in the ‘dictionary’). The latter is the technique used for the word-algebra described above: in the <code>king – man + woman = ?</code> example, the word <code>queen</code> is returned as a result of a search for the nearest vector once the mathematical calculation for <code>king – man + woman</code> has been done. This method would be the one I’d use for the Scriptych word-retrieval mechanism: the dancers’ movements would be converted into vectors, then the nearest word would be returned from the library. However, I would have to make significant changes to improve the quality, and reduce the quantity of words available in order to be able to retrieve any from human movements with a degree of accuracy. Doing this would entail devising a new way to conceptualist movement in space.</p>
<p>One of the technical problems to solve was precisely how the dancers’ moves would be translated into vectors. This was something developed through conversation with the choreographer and dancers, and a period of iterative design and testing in the Opera. After a few meetings, I was paired with the choreographer Simon Valastro.<sup id="fnref:49"><a class="footnote-ref" href="#fn:49">49</a></sup> Whilst approaching the project from two different angles, and with different levels of knowledge about each other’s disciplines, we shared common principles about the rules of the project: we both wanted to work on a project that would push both of us out of our comfort zones, with a high risk of failure; the performance would contain a narrative structure that could be interpreted by an audience, rather than simply being abstract dance; and we were both keen that the technology involved would have to really work, in real-time in front of an audience.</p>
<p>My initial design for the word-retrieval system was laughably absurd<sub>a</sub>. It used the gyroscope data of an X-OSC board (a small wireless interface board with gyroscope and accelerometer built-in, which functioned much like the popular Arduino Uno) and took 100 triple-axis measurements at intervals of 100ms to generate a 300-dimension vector, which could then be used to retrieve a word. This meant that the dancers would have to be consistent in their movements, in each of three movement axes, thirty times in a row over three seconds (I conceived of this method since the introductory word-vector examples all encode words in 300 vectors). However, this level of precision is far greater than even the best industrial robot may be able to produce. It was comically bad. I began to meet Simon regularly morning to make the mechanisms responsive to the way he actually moved.</p>
<p>Each day, I would visit Simon’s studio in the Opera in the morning before his rehearsals so we could play with the latest version of the software. We would test the interface, and try to develop movements which would work. Then, after lunch, whilst he had to rehearse for the ballet he was working on (William Forsythe’s <em>Creation</em>), I would work on improving the code in line with new ideas from the day’s rehearsal. Our lunches proved fertile spaces for conversation about theoretical changes that could be made, which I would build though the afternoon and evening. This proved to be an extremely productive way of developing the interface, and the end result was a highly useable word-retrieval system that could be easily customised to suit the physiology of each user.</p>
<p>The two main barriers to achieving a usable interface point were the reduction of the size of the word library that Gensim/Word2vec created, and the number of vectors each word contained. Simon and I discussed the cinematic nature of the choreography we wanted to develop: the balconies of our site, seen from the opposite side of the Grand Hall, were at a 16:9 ratio, the same as a film screen. Our audience would be international, and therefore, any project presenting spoken words would have to use a vocabulary that could be easily understood. We decided to generate a new library of words taken exclusively from Hollywood film dialogue, using the Cornel Movie Dialogues Corpus.<sup id="fnref:50"><a class="footnote-ref" href="#fn:50">50</a></sup> This resulted in a new library of 17,067 words, many more of which were far more intelligible than those generated earlier.<sup id="fnref:51"><a class="footnote-ref" href="#fn:51">51</a></sup> In addition, we realised that since our purposes for using the vector-space were different to most computer scientists – we would not be doing any complex word arithmetic or calculations – we would be able to encode the words in only three dimensions, instead of three hundred. This meant that we now had a three-dimensional database of words, a concept that is far easier to comprehend since both Simon and I were used to dealing in the three dimensions of the physical world.</p>
<p>The next challenge was to create a mode of locating words within this three-dimensional space. The simplest solution would have been to simply make the dancers move to access a physical position and touch a particular position, for example, with their fingertips, and perhaps use a depth-tracking sensor such as a Microsoft Kinect to measure the dancers. But there are limitations to this mode of movement, and we opted not to use this technique for several reasons. Firstly, the field of range for such sensors is not very large, and therefore the dancers would likely be restricted to a virtual ‘cube’ around their bodies. Secondly, this mode of interaction is a lot like the now tired touch-screen interfaces which have populated sci-fi films such as <em>Minority Report</em> for at least the past fifteen years. A dance using an interface like this would simply be a rehashing of old ideas. And thirdly, the hardware required to make this happen would have to be fixed in place, in some way interfering with and ‘technologizing’ the Opera Garnier.</p>
<figure>
  <a name="figure2-24"></a>
  <img src="../images/figure2-24.jpg" width="100%">
  <figcaption>Figure 2-24:  Scriptych performance in progress at Opera Garnier, as seen from the ‘control booth’. Photo by Justine Emard.</figcaption>
</figure>

<p>As such, we worked with a different measurement of movement: the accelerometers and gyroscopes from a pair of iPhones. On one of our earliest meetings, Simon and I had switched to using iPhones instead of the X-OSC board. This had initially been a matter of convenience – the X-OSC had required a fiddly and unreliable setup process each time we met, which wasted a lot of our time. But both of us always had our iPhones to hand, which contained a multitude of sensors and wireless capacity which we would be able to use at any time. Using the app GyrOSC, we were able to take an iPhone’s internal sensor data and convert this to streaming Open Sound Control (OSC) messages.<sup id="fnref:52"><a class="footnote-ref" href="#fn:52">52</a></sup> The messages could then be received and interpreted by an instance of Max software running on my laptop computer, ready to be interpreted and used to loop up words in the database. A further useful function of the GyrOSC software was the ability to remotely trigger a short vibration from the computer, providing a means of invisible haptic feedback to the dancers. There were a few technical issues which we managed to resolve: the sensors in the different models of iPhone we had were calibrated differently, so the same movement gave different results or was prone to ‘slippage’ (which we resolved by replacing one of the phones for a newer, more accurate model). We also devised a means of affixing a phone to a dancers’ body after much trial and error: phones would be inserted into a small neoprene pouch (made by the Opera’s costume department) with a Velcro panel, the phones would be strapped to the underside of a dancers’ wrist via a black elasticated sports bandage. We settled on this location as it offered a wide range of articulation to be measured, yet enabled the phone itself to be relatively hidden. The use of the neoprene pouches hid the phones themselves, thus preventing the work being read as an advert for a certain technology – or even appearing to be phones. Furthermore, the addition of the sports-bandages enabled the development of a ritualistic process of strapping for each dancer, which we would integrate into the first ‘act’ of the performance.</p>
<p>For the vector-library interface, I developed a novel technique of converting the dancers’ movements to words. Each dancer would have a phone strapped to their right wrist, which would continuously transmit their respective live gyroscope and accelerometer data across a Wi-Fi network to my laptop. This data, labelled <code>d1</code> or <code>d2</code> depending on the phone, would be picked up via Max software. Max would then map these numbers to a range of values. However, these values would only be recorded if Max was in a designated ‘listen’ mode.</p>
<p>The <code>x/y/z</code> gyroscope information read by Max was converted to an integer from 0-3 (4 total values). This meant that any angle reached by the dancers’ wrists could be transformed to a three-part array, such as <code>[0 0 0]</code> or <code>[3 1 2]</code>, and the thresholds for mapping the data were adjusted to suit each dancers’ range of movements.<sup id="fnref:53"><a class="footnote-ref" href="#fn:53">53</a></sup> The accelerometer readings indicated the phones’ current movement. If this fell below a threshold for a designated period of time – one musical ‘count’ – Max would add the current three-part gyroscope integer array to its current ‘move’ – signalled to the dancer via a brief vibration of the phone on their wrist. If the accelerometer recorded no movement for two ‘counts’ the move would finish (signalled to the dancers via two brief phone vibrations), and be converted to a Genism Word2vec query which could return a word.</p>
<p>Each three-part array is called a move, and the string of moves is called a sequence. A move may be <code>[2 2 3]</code> or <code>[2 0 1]</code> and a sequence may be constructed of one move, such as <code>[3 1 0]</code> (<code>night</code>) or <code>[2 2 0], [0 2 3], [3 0 2]</code> (<code>computer</code>) or even <code>[2 2 1], [1 1 3], [0 1 3], [2 0 0], [0 0 1], [1 1 1], [1 0 2], [0 2 2], [2 3 1], [0 2 3], [2 2 3], [2 3 3]</code> (<code>sensible</code>). Most sequences in the final performance consisted of between 1 and 3 moves.</p>
<p><a name="table2-3"></a></p>
<table>
<thead>
<tr>
<th>WORD</th>
<th>VECTOR</th>
<th>MOVE</th>
</tr>
</thead>
<tbody>
<tr>
<td>around</td>
<td><code>[-0.83194745 2.54594374 -0.9201659 ]</code></td>
<td><code>[ 1. 3. 1.]</code></td>
</tr>
<tr>
<td>bris</td>
<td><code>[-0.10030068 0.147881 -0.1577425 ]</code></td>
<td><code>[ 1. 2. 1.]</code></td>
</tr>
<tr>
<td>die</td>
<td><code>[ 3.87849402 -1.2009213 1.25881267]</code></td>
<td><code>[ 3. 1. 2.]</code></td>
</tr>
<tr>
<td>down</td>
<td><code>[-2.75925088 4.41695166 -0.60041034]</code></td>
<td><code>[ 0. 3. 1.]</code></td>
</tr>
<tr>
<td>inventory</td>
<td><code>[ 0.24513638 0.24457133 -0.24679211]</code></td>
<td><code>[ 2. 2. 1.]</code></td>
</tr>
<tr>
<td>lied</td>
<td><code>[ 1.12106276 1.20003092 1.18674994]</code></td>
<td><code>[ 2. 2. 2.]</code></td>
</tr>
<tr>
<td>lot</td>
<td><code>[ 3.77520919 -1.45847201 -1.5530647 ]</code></td>
<td><code>[ 3. 1. 1.]</code></td>
</tr>
<tr>
<td>night</td>
<td><code>[ 3.83613348 -1.4175818 -3.80468154]</code></td>
<td><code>[ 3. 1. 0.]</code></td>
</tr>
</tbody>
</table>
<figure>
  <figcaption>Table 2-3:  Some one-position words from Scriptych.</figcaption>
</figure>

<figure>
  <a name="figure2-25"></a>
  <img src="../images/figure2-25.jpg" width="100%">
  <figcaption>Figure 2-25:  Opera Garnier de Paris. Photograph by the author.</figcaption>
</figure>

<p>The transformation of these sequences to spatial navigation is one part of the technical innovation within this project, and the best of my knowledge, represents an original innovation. The technique is as follows: the 17,067 words are stored as three-dimensional vectors within a database. This database is virtually represented within a cube, with an uneven distribution of words generated by Gensim/Word2vec. Each word therefore can be represented as a fixed point within this cube, so that searching for <code>[0.96784878, 0.98643523, -0.0787757]</code> would return the word ‘absurd’. The job of the phone that the dancers wear on their wrist is to translate their movements into spatial navigation within the virtual cube, thus moving to the coordinates that best represent the desired word. The challenge was creating a reliable, replicable means by which the dancers’ movements could be translated to this spatial navigation within the virtual cube, using only the rotational data from the iPhone’s built-in gyroscopes (which can only record rotational angles along three axes). In other words, a phone’s rotational data would have to translate to virtual spatial movements. The resolution of the gyroscopes was down-sampled so that each angle was represented by an integer from 0-3. The sequences that the dancers perform represent a mode of navigating this space, with each ‘move’ in a sequence representing zooming in to a more specific part of the cube. Each ‘move’ represents subdividing the current cube into a 4x4x4 array of smaller cubes. A move to <code>[0 0 0]</code> would select the near most bottom left sub-cube, whilst <code>[3 3 3]</code> would select the cube furthest top right. Multi-part sequences apply this technique recursively, so that each move within a sequence represents a smaller ‘zoom’ by an order of four. The final word results from a nearest-word search from the central most point of the final cube of the navigation. This navigation technique allows the user to move to a small set of positions (four positions in each axis) yet generate a 3d position with the degree of accuracy necessary for any necessary word in a relatively small number of moves.</p>
<figure>
  <a name="figure2-26"></a>
  <img src="../images/figure2-26.jpg" width="100%">
  <figcaption>Figure 2-26:  TouchOSC interface for a controlling system readiness and music during performances. Photograph by the author.</figcaption>
</figure>

<p>In order to be able to predict the words that dancers would be dancing with – and provide them with a written performance script to dance to – I wrote a Python programme to map the necessary navigation for each word. Most common words took 1-3 moves per sequence to reach, which was far easier than the initial hypersensitive prototype I had shown Simon previously. Both dancers would be ‘speaking’ words via their movements (see Figure 2-27). Given the irregular spacing of the words in the three-dimensional space (due to the clustered method of specialisation by the Word2vec functions), a few of the initial sub-cubes were ‘empty’. One of these was used as a ‘flush’ position, which would enable the dancers to clear the current array if they suspected they had hit the wrong position. Since most useful words can be reached within five moves the act of creating a word generally takes around 3-5 seconds.</p>
<p>Here are some one-position move examples:</p>
<p>I believe this method of navigation – translating three-axis arm rotation into three-dimensional, scaling movements through an array of increasingly small cubes in order to retrieve particular vectors – is novel.</p>
<p>I created a dialogue for the performance, which would act as its script, using only words accessible within 3 move sequences. The dialogue was about the impossibility of conversation – in equal part inspired by the direct experience I had trying to maintain communications with my partner across time-zones with faltering digital connectivity, and by the central tenet of Gordon Pask’s Conversation Theory, which I believe has roots that are in line with absurd<sub>b</sub> principles. Pask’s 1976 book <em>Conversation Theory</em> discusses the mechanisms by which conversations take place. Since concepts are formed by minds based on experiences, and experiences are subjective, Pask argues, all communication between two sentient beings can hope to do is reach a mutually satisfying definition of the same concept.<sup id="fnref:54"><a class="footnote-ref" href="#fn:54">54</a></sup> This renders <em>true</em> and total communication impossible; although, much like a machine which can pass the Turing Test, there are ways to ensure that the other mind is understanding <em>enough</em> of what one is saying to get by. The dialogue I wrote was designed to appear as a conversation between two people separated from each other, but in reality both are more interested in their own viewpoints than listening to one another. The structure is palindromic: one dancer’s lines are merely the others’, reversed.</p>
<p><a name="figure2-27"></a></p>
<table>
<thead>
<tr>
<th>DANCER A</th>
<th>DANCER B</th>
</tr>
</thead>
<tbody>
<tr>
<td>a fragment</td>
<td>-</td>
</tr>
<tr>
<td>all that remains</td>
<td>-</td>
</tr>
<tr>
<td>a conversation</td>
<td>hello?</td>
</tr>
<tr>
<td>we talk</td>
<td>hello?</td>
</tr>
<tr>
<td>we listen</td>
<td>can you hear me?</td>
</tr>
<tr>
<td>we don’t listen</td>
<td>we don’t listen</td>
</tr>
<tr>
<td>can you hear me?</td>
<td>we listen</td>
</tr>
<tr>
<td>hello?</td>
<td>we talk</td>
</tr>
<tr>
<td>hello?</td>
<td>a conversation</td>
</tr>
<tr>
<td>-</td>
<td>all that remains</td>
</tr>
<tr>
<td>-</td>
<td>a fragment</td>
</tr>
</tbody>
</table>
<figure>
  <figcaption>Figure 2-27:  Scriptych performance script.</figcaption>
</figure>

<p>Several of the words required multi-position moves; in tests, these slowed the pace of the dialogue and made it hard for an audience to understand that the dancers were talking to each other; each extra position added another couple of seconds and slowed the pace, and intelligibility of dialogue between the dancers down. However, due to the uneven spatial distribution of words in the dictionary, many of the initial 64 ‘positions’ that the dancers could move to were ‘open’. In order to speed the word-retrieval up, we chose to create ‘shortcut’ moves within several of these positions, so that all of the words in the dialogue would be single-position movements. We also decided that the delivery of these words should be spoken as a sentence, so that dancers were assembling chunks of text, rather than individual words (which had a strange, floating, appearance in tests, and made following dialogue hard). This required building an anticipatory ‘listen’ function within Max that would assemble strings of words into sentences, something that would require the construction of a control-interface.</p>
<p>Working over the period of around a month, Simon and I progressed from the development of a viable interface to the development of a viable performance. We decided to give the performance a three-part structure, so that the audience would firstly see two dancers on the opposite balconies moving with the distinctive visual grammar that Simon had developed to suit the interface. Two suited men would then enter the balconies, and wrap their arms with the straps whilst positioning the iPhones. This second phase would then show the dancers ‘learning’ to talk using their movements: generating simple words from movements, which would be played through speakers on their side of the Grand Hall. This would lead to their attempts at communication, working through the dialogue script I had written. Then, in the third phase, signalling frustration at their inability to effectively communicate, the performance would descend into a wall of unintelligible, manic and overlapping speech, accompanied by frantic and rapid choreography. This would be balanced by a slower reprise, where the first two lines of the dialogue (‘all that remains / a fragment’) would be repeated by the female dancer. The three acts of highly scripted movements interacting with a computer, would be entitled Scriptych; a mistranslated play on the word triptych (meaning three associated works intended to be appreciated together). The misrepresentation of the concept of the triptych, in which the works are usually not connected, is a conscious reflection of the miscommunication caused by translation throughout the piece.</p>
<p>The differing stages of the performance required the construction of a control interface, so that the ‘listening’ functions of the Max programme would be searching for the right number of words at the right times. There was a difference between the stage when the dancers were dancing without phones (which should produce no words), the stage where the dancers were creating one word at a time, the period of scripted dialogue, and the frenetic section. The data processing would all occur via a ‘hub’ of Max running on my computer. Max would constantly receive input with accelerometer and gyroscope readings from the dancers’ two iPhones, as well as signalling input from an iPhone and iPad that Simon and I would operate to control the overall performance. These control interfaces would also use OSC to send and receive signals – in this case, running bespoke interfaces through the app TouchOSC.<sup id="fnref:55"><a class="footnote-ref" href="#fn:55">55</a></sup> From the iPad I could control the reading behaviour that Max would use to listen for the dancers’ movements. I could also re-orient the gyroscopes on the dancers’ arms, which would misalign after the period of intense freneticism in part 3 of the dance. We also divided the performance into eleven periods with different associated musical levels (which is described below). OSC was also used internally in the laptop to communicate between Max and Python – the use of one communication protocol for the entire performance simplified the workflow.<sup id="fnref:56"><a class="footnote-ref" href="#fn:56">56</a></sup></p>
<h3 id="soundtrack_1">Soundtrack<a class="headerlink" href="#soundtrack_1" title="Permanent link">#</a></h3>
<p>At the same time as the initial gyroscope readings, I started to build software to generate music for the performance. This was the second project I had developed using Max software, after <em>24fps Psycho</em> (see chapter 3). The visual diagramming mode it uses to programme became intuitive over the months I was using it for, and it enabled multiple conversations to occur between Simon and myself about fairly abstract computing, using the visual language of the software (by the end of the process, Simon was able to correct some of my mistakes). The OSC protocol had been developed as a more advanced version of MIDI, allowing the lightweight transmission of streaming music data. It seemed only logical to use this same protocol to generate music.</p>
<p>The initial music software was an <em>arpeggiator</em> – a piece of software which would play ascending and descending notes within a chord which would be generated from the OSC inputs. The aesthetic of this music was inspired by the recent Radiohead single <em>Burn the Witch</em>, and the orchestral work of Johnny Greenwood in the soundtrack to the film <em>There Will Be Blood</em>; I liked the tension created by the staccato string sections in both.<sup id="fnref:57"><a class="footnote-ref" href="#fn:57">57</a></sup> I also thought the D# minor scale used in <em>Burn the Witch</em> created a high level of suspense, which would be well-placed within this project. The arpeggiator would be based on a 4/4 timing. Initially, I built a primitive scale-finder which would identify all possible notes within a given scale. The three axes of the iPhone’s gyroscope, <code>x/y/z</code>, would be used to select a bass, middle, and high note from four possible noted of each, based on thresholds within the incoming signal. If the phone’s base rotation was 0-89º, it would play one note, 90-179º, the next note, and so on. The three axes generated three notes in a chord (low, middle and high). A numerical selector enabled the user to choose how many notes would play each bar: 0-4 (the fourth note would be the second note, repeated). The notes generated in Max would then trigger a staccato string sound in Ableton Live music generation software.</p>
<p>The arpeggiator alone would not be enough to fill the entire entrance to Opera Garnier for nine minutes. Fortunately, the Pavillon was already working with the Institute National d’Audiovisuel (INA), and for the evening of performance we would be using equipment and engineers from the GRM, INA’s experimental music division. I was also able to work with Julien Perez, a former Pavillon resident and composer in the GRM’s studios. By this time, Simon and I had already discussed at length the ‘aesthetic’ the soundtrack was to have. The GRM sound setup consisted of 8 highly controllable speakers throughout the room, and sophisticated software to enable sound to be highly spatialised. Julien and I created a drum beat which travelled from speaker to speaker with every hit; another, more intense, drum track; a ‘heartbeat’; two bass tracks; and a record-hiss style white noise track. These were all designed to work with the same timing structure, and the timing of the measurements of the phone movements, so that the dancers could measure their movements according to the ambient music. The music would be layered in a pre-choreographed order, so that the entire performance went through different emotional stages, from a slow, white noise-and-heartbeat-laden beginning, to a strings-accompanied arpeggiating (generated by one of the dancers’ movements), slightly heavier middle section with the dialogue (generated by the dancers), to a period of intense movement, music and heavily overlaid speech (generated by the dancers). This penultimate stage was then followed by a moment of calm and silence, a pause by the dancers to gather their breath, and a slow return to fragmentary words of dialogue and a subtler iteration of the music from the second part.</p>
<p>The creation of the soundtrack brought a range of technical issues to light. Firstly, through early tests of the arpeggiator in Simon’s studio, we had found that the best way to build tension in the performance was to incrementally increase the number of notes the arpeggiator played, so that instead of progressing from nothing to a full 4-note arpeggiated chord, we would start playing only the first note of the bar, then the first two, and so on. We could then use a similar sound motif later in the performance to signal its closure. Since the processing of signals for the performance itself would take place on my computer, it made sense to also use it for part of the soundtrack – the voices, and synthesised, arpeggiated viola notes. However, on the evening of the performance, the GRM would install the 8-part speaker array into the Opera, which would offer the spatialised soundtrack. The interface for the speaker array was a large, specially built mixing desk with numerous inputs and outputs – but without a prolonged period for hardware and software testing, we had to compromise on the playback technique for the music. During the performance, the rest of the soundtrack would be played back as a ‘sound bed’: multiple channels of pre-recorded audio which could be faded in and out as appropriate. We developed a basic mode of synchronising playback so that all of the tracks would be in time with the generated music from my computer. The soundtrack would move between ‘sections’ through Julien’s adjusting the playback levels (in reality this was done via pre-set level positions for each section, set on the desk).</p>
<p>This live music generation caused a slight challenge: the other soundtracks that would play during the evening would all be exclusively pre-recorded or live. These pre-recorded tracks would be played through a dedicated computer with audio outputs into a large mixing desk, with adjustable output volumes. My soundtrack was different, mixing live audio input – which had to be in time – with a spatialized pre-recorded soundtrack. Julien and I recorded a complementary 8-part soundtrack in the same time signature as the arpeggiator (120 bpm 4/4). This consisted of: a low, throbbing hum; a ‘heartbeat’ drum noise every 8 beats; a non-spatialised drum beat; a spatialised fast drum beat which circled the 8 speakers; and a spatialised bassline which would appear to move around the room. These sounds could be combined, along with the generated voices and arpeggiator, to create varying levels of tension and freneticism without changing the underlying time-signature. Before each performance, I would synchronise my computer’s arpeggiator with the pre-recorded soundtrack on Julien’s computer.</p>
<h3 id="performance"><span id="_Toc475104148" class="anchor"><span id="_Toc475104566" class="anchor"></span></span>Performance<a class="headerlink" href="#performance" title="Permanent link">#</a></h3>
<p>We worked with two dancers for a period of several weeks before the performance at the Opera. This enabled us to adjust the physical parameters of the readings to suit the dancers. The method that both dancers employed demonstrated to me some of the myriad ways in which dancers can interpret instruction, and how people understand technical systems. Simon had worked with the limited palette offered by the software and hardware we were using to create a choreography based around the short dialogue. The storyline was simple: a male and female dancer walked onto a balcony on the left and the right of the grand staircase in the Opera. They began, individually, to perform moves in a morose manner, unaware of each other’s presence, as if they are practicing on their own. After a few minutes, they adopt a standing position; a suited man approaches each dancer and applies a device to their arm with a black strap. The music changes; the female dancer starts moving, whilst the male dancer remains still. Both dancers’ movements begin to create individual fragmentary words (in reality this part was improvised each night, with dancers choosing their favourite positions). A subtle music cue causes the dancers to pause. The female dancer then performs a few lines of dialogue; the male dancer appears to respond, offering a greeting. Both of the dancers, although they appear to be talking to each other, are positioned parallel to each other, facing forwards towards the audience, rather than at each other. It becomes clear that they are not, in fact talking to each other, but rather repeating the same lines forwards and backwards. They become frustrated at their inability to communicate, and lash out, moving erratically and frantically, both spewing words at a frenetic rate. Finally, they stop, catch their breath, and attempt a conversation again. It is futile. They leave the stage to the lines:</p>
<blockquote>
<p><code>all that remains</code></p>
<p><code>a fragment</code></p>
</blockquote>
<figure>
  <a name="figure2-28"></a>
  <img src="../images/figure2-28.png" width="100%">
  <figcaption>Figure 2-28:  Dancers Eve Grinsztajn and Mathieu Contat practice Simon Valastro’s choreography for Scriptych at Opera Garnier. Photo by the author.</figcaption>
</figure>

<h3 id="film">Film<a class="headerlink" href="#film" title="Permanent link">#</a></h3>
<p>The dancers were filmed from above, in order to superimpose their movements on one another during video editing. The film finally unites the two dancers on one screen, although as they appear to variously embrace and move over the top of one another, they display the physical disconnection that was present in the live performance. The aerial view also dehumanises the view of the dancers, so that we can see their muscles and physical movements, but not the expressions on their faces.<sup id="fnref:58"><a class="footnote-ref" href="#fn:58">58</a></sup> This makes the film more in line with the disembodied, meaningless communication experience it is meant to convey.</p>
<figure>
  <a name="figure2-29"></a>
  <img src="../images/figure2-29.png" width="100%">
  <figcaption>Figure 2-29:  Performances from both dancers superimposed onto each other, revlealing inherent differences in the way that both dancers move. See supporting material for video form of this. Photography by the author.</figcaption>
</figure>

<p>As a side note, part of the initial work training this new library included cleaning up thousands of lines of codified, tokenised, Hollywood film dialogue. As part of the process, I alphabetised the entire corpus of dialogue lines. Reading through the data, I was struck by how oddly poetic these disparate statements were, connected only by their appearance in one of many films, and the first letter (and often word) of the line. I used several of the lines from this data to create a series of Oulipian-style poems. One of these, entitled ‘<em>48 lines about love from Hollywood films, alphabetically ordered</em>’, can be found in appendices.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Both of these projects were written in Processing and interfaced with Arduino microcontrollers.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>Both of these projects were written extensively in Python; <em>24fps Psycho</em> also used a Max interface to play videos live. Cycling ’74 and IRCAM, <em>Max 7</em>, version 7.3.1, Apple Macintosh (Cycling ’74 / IRCAM, 2016), 7.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>John K Ousterhout, ‘Scripting: Higher Level Programming for the 21st Century’, <em>Computer</em> 31, no. 3 (1998): 23–30.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>This statement comes through first-hand experience visiting and teaching at various design and architecture schools internationally.&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:5">
<p>See Peg Rawes, ‘Spinoza’s Geometric and Ecological Ratios’, in <em>The Politics of Parametricism: Digital Technologies in Architecture</em>, ed. Matthew Poole and Manuel Shvartzberg (London and New York: Bloomsbury Academic, 2015).&#160;<a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:6">
<p>I was introduced to state transition diagrams by Stephen Gage, Richard Roberts and Ruairi Glynn during my masters in architecture.
<em>Pseudo-code</em> is a means of planning what computer code will do in advance, using natural language or sentences to describe what the eventual code <em>should</em> do. Often, my pseudo-code remains in the final programme, but in commented form – that is, human-readable but ignored by the machine compiler.&#160;<a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:7">
<p>The name PHP is somewhat of a programming joke: it is a ‘recursive acronym’ for PHP: Hypertext Processor. The PHP Group, ‘What Is PHP?’ (The PHP Group), accessed 6 December 2016, http://php.net/manual/en/intro-whatis.php.&#160;<a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:8">
<p><em>The Zen of Python</em> is a collection of ‘20’ principles that Python devotees should strive towards, written by programmer Tim Peters in 1998. The part-humorous, part-serious principles are ‘baked in’ to the language in the form of an Easter Egg (a hidden ‘bonus’ feature within a regular programme); running the command <code>import this</code> into the interpreter returns the list of principles. ‘PEP 20 -- The Zen of Python’, <em>Python.org</em>, accessed 7 December 2016, https://www.python.org/dev/peps/pep-0020/.
Python itself is named after Monty Python’s Flying Circus; the fact that there are only 19 principles in the 20 principles is in itself a reflection of the Monty Python-esque humour found throughout the documentation of the language. The Python Software Foundation, ‘General Python FAQ’ (The Python Software Foundation), accessed 6 December 2016, https://docs.python.org/2/faq/general.html#why-is-it-called-python.&#160;<a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:9">
<p>Through this working, I might calculate that it would take 40 seconds to reach the desired temperature from a starting point of 10º, 15 seconds from 15º, 60 seconds from 20º, 90 seconds from 21º, and so on.&#160;<a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn:10">
<p>Italo Calvino, ‘Cybernetics and Ghosts’, in <em>The Literature Machine: Essays</em>, trans. Patrick Creagh (London: Secker and Warburg, 1987), 3–27.
Note that Calvino’s allusion to DNA code is distinct from that used by parametricists to rationalise their craft, which has been heavily criticised by Rawes in <em>Spinoza's Geometric and Ecological Ratios</em>. Calvino’s point was that basic combinatorics, as found in DNA (which consists of only four bases) has been used by nature to create the entire gamut of known life. Rawes, ‘Spinoza’s Geometric and Ecological Ratios’.&#160;<a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
<li id="fn:11">
<p>Calvino, ‘Cybernetics and Ghosts’, 11.&#160;<a class="footnote-backref" href="#fnref:11" title="Jump back to footnote 11 in the text">&#8617;</a></p>
</li>
<li id="fn:12">
<p>Ibid., 10.&#160;<a class="footnote-backref" href="#fnref:12" title="Jump back to footnote 12 in the text">&#8617;</a></p>
</li>
<li id="fn:13">
<p>Italo Calvino, ‘The Burning of the Abominable House’, in <em>Numbers in the Dark and Other Stories</em>, ed. Esther Calvino, trans. Tim Parks, Array (New York: Pantheon Books, 1995), 156–69.&#160;<a class="footnote-backref" href="#fnref:13" title="Jump back to footnote 13 in the text">&#8617;</a></p>
</li>
<li id="fn:14">
<p>Esther Calvino, ‘Preface’, in <em>Numbers in the Dark and Other Stories</em>, by Italo Calvino, trans. Tim Parks, Array (New York: Pantheon Books, 1995), 2.&#160;<a class="footnote-backref" href="#fnref:14" title="Jump back to footnote 14 in the text">&#8617;</a></p>
</li>
<li id="fn:15">
<p>A. M. Turing, ‘On Computable Numbers, with an Application to the Entscheidungsproblem’, <em>Proceedings of the London Mathematical Society</em> 42, no. 2 (1936): 230–65; Prof Jack Copeland Lee Dave, ‘Alan Turing: The Codebreaker Who Saved “Millions of Lives”’, <em>BBC News</em>, 19 June 2012, sec. Technology, <a href="http://www.bbc.com/news/technology-18419691">http://www.bbc.com/news/technology-18419691</a>.&#160;<a class="footnote-backref" href="#fnref:15" title="Jump back to footnote 15 in the text">&#8617;</a></p>
</li>
<li id="fn:16">
<p>Alan M. Turing, ‘Digital Computers Applied to Games’, in <em>Faster than Thought: A Symposium on Digital Computing Machines</em>, ed. Bertram Vivian Bowden (London: Sir Isaac Pitman and Sons, Ltd, 1953), 288.&#160;<a class="footnote-backref" href="#fnref:16" title="Jump back to footnote 16 in the text">&#8617;</a></p>
</li>
<li id="fn:17">
<p>Fortunately the programme was able to be tested on Manchester University’s machine several months after Turing’s original paper. Turing notes that when tried on computer, the ‘technique of programming was rather crude, and many requirements, increasing the speed of operation, are possible.’ Ibid., 296.Ibid., 296.(Alan M. Turing 1953, 296)This led to a computer that was ‘disappointingly slow when playing chess – in contrast to the extreme superiority over human computers where purely mathematical problems are concerned.’ Ibid.&#160;<a class="footnote-backref" href="#fnref:17" title="Jump back to footnote 17 in the text">&#8617;</a></p>
</li>
<li id="fn:18">
<p>Alan M. Turing, ‘Digital Computers Applied to Games’, 289.&#160;<a class="footnote-backref" href="#fnref:18" title="Jump back to footnote 18 in the text">&#8617;</a></p>
</li>
<li id="fn:19">
<p>Nancy Katherine Hayles, <em>My Mother Was a Computer: Digital Subjects and Literary Texts</em> (Chicago and London: The University of Chicago Press, 2005); David Allen Grier, <em>When Computers Were Human</em> (Princeton and Oxford: Princeton University Press, 2005).&#160;<a class="footnote-backref" href="#fnref:19" title="Jump back to footnote 19 in the text">&#8617;</a></p>
</li>
<li id="fn:20">
<p>An pair of adverts in the New York Times dating from 1892 and 1893 demand ‘Human Computer Wanted’ (a curious first word, as there were no other types of computer at the time). Private advertiser, ‘A Computer Wanted’, <em>New York Times</em>, 2 May 1892, sec. Advertisements; Private advertiser, ‘Government Computers Wanted.’, <em>New York Times</em>, 1 January 1893, sec. Advertisements.&#160;<a class="footnote-backref" href="#fnref:20" title="Jump back to footnote 20 in the text">&#8617;</a></p>
</li>
<li id="fn:21">
<p>‘Computer, N.’, <em>OED Online</em> (Oxford University Press), accessed 9 November 2013, <a href="http://www.oed.com/view/Entry/37975">http://www.oed.com/view/Entry/37975</a>; Grier, <em>When Computers Were Human</em>, 5.&#160;<a class="footnote-backref" href="#fnref:21" title="Jump back to footnote 21 in the text">&#8617;</a></p>
</li>
<li id="fn:22">
<p>Tung-Hui Hu argues that the concept of <em>the cloud</em> is, in fact, a construction and an extension of pre-existing power structures. An interesting socio-historical analysis of this argument can be found in Tung-Hui Hu, <em>A Prehistory of the Cloud</em>, First (Cambridge, MA: MIT Press, 2015).&#160;<a class="footnote-backref" href="#fnref:22" title="Jump back to footnote 22 in the text">&#8617;</a></p>
</li>
<li id="fn:23">
<p>The Daily Mail runs regular Artificial Intelligence scare stories which veer from prophetic doom-mongering to the comically absurd<sub>a</sub>. Headlines from recent years include: ‘<em>Artificial Intelligence is as dangerous as NUCLEAR WEAPONS: AI pioneer warns smart computers could doom mankind’</em> and ‘<em>Could robots turn people into PETS? Elon Musk claims artificial intelligence will treat humans like 'labradors'</em>’. Richard Gray, ‘Artificial Intelligence as Dangerous as Nuclear Weapons’, <em>Mail Online</em>, 17 July 2015, <a href="http://www.dailymail.co.uk/sciencetech/article-3165356/Artificial-Intelligence-dangerous-NUCLEAR-WEAPONS-AI-pioneer-warns-smart-computers-doom-mankind.html">http://www.dailymail.co.uk/sciencetech/article-3165356/Artificial-Intelligence-dangerous-NUCLEAR-WEAPONS-AI-pioneer-warns-smart-computers-doom-mankind.html</a>; Ellie Zolfagharifard, ‘Could Robots Turn People into PETS? Elon Musk Claims Artificial Intelligence Will Treat Humans like “Labradors”’, <em>Mail Online</em>, 25 March 2015, <a href="http://www.dailymail.co.uk/sciencetech/article-3011302/Could-robots-turn-people-PETS-Elon-Musk-claims-artificial-intelligence-treat-humans-like-Labradors.html">http://www.dailymail.co.uk/sciencetech/article-3011302/Could-robots-turn-people-PETS-Elon-Musk-claims-artificial-intelligence-treat-humans-like-Labradors.html</a>.<br>The former headline was accompanied by a still image from the film <em>Terminator</em>; headlines such as this only serve to perpetuate pre-existing, polemic attitudes towards a complex set of technologies and philosophical positions. James Cameron, <em>The Terminator</em>, 1984.&#160;<a class="footnote-backref" href="#fnref:23" title="Jump back to footnote 23 in the text">&#8617;</a></p>
</li>
<li id="fn:24">
<p>Google’s London-based artificial intelligence company <em>DeepMind</em>, acquired in January 2014, recently used their <em>AlphaGo</em> algorithm to beat professional player Lee Sedol at four of five games of Go. Paul Marks, ‘Google Buys AI Firm DeepMind to Boost Image Search’, <em>New Scientist</em>, 27 January 2014, <a href="https://www.newscientist.com/article/dn24946-google-buys-ai-firm-deepmind-to-boost-image-search/">https://www.newscientist.com/article/dn24946-google-buys-ai-firm-deepmind-to-boost-image-search/</a>; David Silver et al., ‘Mastering the Game of Go with Deep Neural Networks and Tree Search’, <em>Nature</em> 529, no. 7587 (28 January 2016): 484–89.<br>Google and Facebook have also both released machine learning platforms, called <em>TensorFlow</em> and <em>Big Sur</em> respectively, which allow users to experiment with their languages and protocols; this has been interpreted by industry analysis such as Tom Simonite as a cynical move to effectively improve their software through user feedback, identifying new capabilities, and also attract developer talent to the companies themselves. Martín Abadi et al., <em>TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</em>, 2015, <a href="http://tensorflow.org/">http://tensorflow.org/</a>; Kevin Lee and Serkan Piantino, ‘Facebook to Open-Source AI Hardware Design’, <em>Facebook Code</em>, 10 December 2015, <a href="https://code.facebook.com/posts/1687861518126048/facebook-to-open-source-ai-hardware-design/">https://code.facebook.com/posts/1687861518126048/facebook-to-open-source-ai-hardware-design/</a>; Tom Simonite, ‘Why Google, Facebook, Microsoft and IBM Are Desperate to Give Away AI Technology’, <em>MIT Technology Review</em>, 10 December 2015, <a href="https://www.technologyreview.com/s/544236/facebook-joins-stampede-of-tech-giants-giving-away-artificial-intelligence-technology/">https://www.technologyreview.com/s/544236/facebook-joins-stampede-of-tech-giants-giving-away-artificial-intelligence-technology/</a>.&#160;<a class="footnote-backref" href="#fnref:24" title="Jump back to footnote 24 in the text">&#8617;</a></p>
</li>
<li id="fn:25">
<p>Technology companies such as Google and Apple, as well as car manufacturers such as ‘Tesla Motors, Audi, Mercedes-Benz, Volvo, and General Motors’ are all testing self-driving vehicles. Training these vehicles relies heavily on machine learning. Will Knight, ‘What to Know Before You Get In a Self-Driving Car’, <em>MIT Technology Review</em>, 18 October 2016, <a href="https://www.technologyreview.com/s/602492/what-to-know-before-you-get-in-a-self-driving-car/">https://www.technologyreview.com/s/602492/what-to-know-before-you-get-in-a-self-driving-car/</a>.&#160;<a class="footnote-backref" href="#fnref:25" title="Jump back to footnote 25 in the text">&#8617;</a></p>
</li>
<li id="fn:26">
<p>John R. Searle, ‘Minds, Brains and Science: Walk to Patagonia’, <em>Reith Lectures</em> (BBC Radio 4, 28 November 1984), <a href="http://www.bbc.co.uk/programmes/p00gq1fk/episodes/guide">http://www.bbc.co.uk/programmes/p00gq1fk/episodes/guide</a>; John R. Searle, ‘Minds, Brains and Science: Grandmother Knew Best’, <em>Reith Lectures</em> (BBC Radio 4, 21 November 1984), <a href="http://www.bbc.co.uk/programmes/p00gq1fk/episodes/guide">http://www.bbc.co.uk/programmes/p00gq1fk/episodes/guide</a>; John R. Searle, ‘Minds, Brains and Science: A Froth on Reality’, <em>Reith Lectures</em> (BBC Radio 4, 7 November 1984), <a href="http://www.bbc.co.uk/programmes/p00gq1fk/episodes/guide">http://www.bbc.co.uk/programmes/p00gq1fk/episodes/guide</a>; John R. Searle, ‘Minds, Brains and Science: The Freedom of the Will’, <em>Reith Lectures</em> (BBC Radio 4, 12 December 1984), <a href="http://www.bbc.co.uk/programmes/p00gq1fk/episodes/guide">http://www.bbc.co.uk/programmes/p00gq1fk/episodes/guide</a>; John R. Searle, ‘Minds, Brains and Science: A Changing Reality’, <em>Reith Lectures</em> (BBC Radio 4, 5 December 1984), <a href="http://www.bbc.co.uk/programmes/p00gq1fk/episodes/guide">http://www.bbc.co.uk/programmes/p00gq1fk/episodes/guide</a>; John R. Searle, ‘Minds, Brains and Science: Beer Cans and Meat Machines’, <em>Reith Lectures</em> (BBC Radio 4, 14 November 1984), <a href="http://www.bbc.co.uk/programmes/p00gq1fk/episodes/guide">http://www.bbc.co.uk/programmes/p00gq1fk/episodes/guide</a>.&#160;<a class="footnote-backref" href="#fnref:26" title="Jump back to footnote 26 in the text">&#8617;</a></p>
</li>
<li id="fn:27">
<p>John R. Searle, ‘The Chinese Room Argument’, ed. Robert Andrew Wilson and Frank C. Keil, <em>The Mit Encyclopedia of Cognitive Science</em> (MIT Press, 2001), 115.&#160;<a class="footnote-backref" href="#fnref:27" title="Jump back to footnote 27 in the text">&#8617;</a></p>
</li>
<li id="fn:28">
<p>John R. Searle, ‘Minds, Brains, and Programs’, <em>Behavioral and Brain Sciences</em> 3, no. 3 (1980): 417–57. A series of rebuttals and an examination of the implications of the Chinese Room argument can be found in David Cole, ‘The Chinese Room Argument’, ed. Edward N. Zalta, <em>The Stanford Encyclopedia of Philosophy</em> (Metaphysics Research Lab, Stanford University, 2015), <a href="https://plato.stanford.edu/archives/win2015/entries/chinese-room/">https://plato.stanford.edu/archives/win2015/entries/chinese-room/</a>.&#160;<a class="footnote-backref" href="#fnref:28" title="Jump back to footnote 28 in the text">&#8617;</a></p>
</li>
<li id="fn:29">
<p>Cole, ‘The Chinese Room Argument’, 7; Roger C. Schank and Robert P. Abelson, ‘Scripts, Plans, and Knowledge’, in <em>Proceedings of the 4th International Joint Conference on Artificial Intelligence-Volume 1</em> (Morgan Kaufmann Publishers Inc., 1975), 151–157; Roger C. Schank and Robert P. Abelson, <em>Scripts Plans Goals and Understanding: An Inquiry into Human Knowledge Structures</em> (New Jersey: Lawrence Erlbaum Associates, 1977).&#160;<a class="footnote-backref" href="#fnref:29" title="Jump back to footnote 29 in the text">&#8617;</a></p>
</li>
<li id="fn:30">
<p>The Turing Test is a hypothesis put forth by Alan Turing in 1950 which proposes that a computer programme which could convincingly answer questions put to it by a human interrogator would mark a significant advance in artificial intelligence. It has long been a disputed benchmark of artificial intelligence research, and even the inspiration for sci-fi fiction such as the Philip K Dick novel <em>Do Androids Dream of Electric Sheep</em>?, later popularised as the film Blade Runner. A. M. Turing, ‘Computing Machinery and Intelligence’, <em>Mind</em> 59, no. 236 (1 October 1950): 433–60, doi:10.2307/2251299; Graham Oppy and David Dowe, ‘The Turing Test’, ed. Edward N. Zalta, <em>The Stanford Encyclopedia of Philosophy</em> (Metaphysics Research Lab, Stanford University, 2016), <a href="https://plato.stanford.edu/archives/spr2016/entries/turing-test/">https://plato.stanford.edu/archives/spr2016/entries/turing-test/</a>; Philip K. Dick, <em>Blade Runner: (Do Androids Dream of Electric Sheep)</em> (New York: Ballantine, 1982); Ridley Scott, <em>Blade Runner</em> (Warner Bros. Pictures, 1982).&#160;<a class="footnote-backref" href="#fnref:30" title="Jump back to footnote 30 in the text">&#8617;</a></p>
</li>
<li id="fn:31">
<p>Searle, ‘The Chinese Room Argument’, 115.&#160;<a class="footnote-backref" href="#fnref:31" title="Jump back to footnote 31 in the text">&#8617;</a></p>
</li>
<li id="fn:32">
<p>Critics of Searle’s argument include, among others, Ray Kurzweil, Paul and Patricia Churchland, and Andy Clark. Kurzweil argues that Searle’s argument could in fact be focused on human brains: ‘I believe that the scale of Searle’s misrepresentation of ideas from the AI community stems from a basic lack of understanding technology.’ Ray Kurzweil, ‘Locked in His Chinese Room: A Response to John Searle’, in <em>Are We Spiritual Machines?: Ray Kurzweil vs. the Critics of Strong A.I.</em>, ed. Jay Wesley Richards (Seattle: Discovery Institute Press, 2002), 170.; Churchland and Churchland argue that the Chinese Room highlights general ignorance of the mechanisms and workings of human cognition and semantics, a view part-echoed by Clark, who discusses the relationship between Schank and Abelson’s scripts and understanding and what he terms ‘connectionist systems.’ Paul M. Churchland and Patricia Smith Churchland, ‘Could a Machine Think?’, <em>Scientific American</em>, January 1990; Rudi Lutz (eds.) Andy Clark Rudi Lutz (auth.), Andy Clark, <em>Connectionism in Context</em>, 1st ed., Artificial Intelligence and Society (Springer-Verlag London, 1992), 76–78; Cole, ‘The Chinese Room Argument’.&#160;<a class="footnote-backref" href="#fnref:32" title="Jump back to footnote 32 in the text">&#8617;</a></p>
</li>
<li id="fn:33">
<p>I became interested in Turing via a seminar with Ranulph Glanville at the Bartlett in 2009, and my involvement in design and building mechanisms for the Universal Tea Machine in 2012: an Alan Turing-inspired eccentric tea-making calculator conceived by Smout Allen, You and Pea, and Iain Borden, commissioned by the Mayor of London for the Olympics, and fabricated by Westby Jones.&#160;<a class="footnote-backref" href="#fnref:33" title="Jump back to footnote 33 in the text">&#8617;</a></p>
</li>
<li id="fn:34">
<p>Kurt Vonnegut, <em>The Sirens of Titan</em> (New York: Delacorte Press, 1959). The novel was nominated for a Hugo Award.&#160;<a class="footnote-backref" href="#fnref:34" title="Jump back to footnote 34 in the text">&#8617;</a></p>
</li>
<li id="fn:35">
<p>The novel also contains numerous absurd<sub>ab</sub> themes&#160;<a class="footnote-backref" href="#fnref:35" title="Jump back to footnote 35 in the text">&#8617;</a></p>
</li>
<li id="fn:36">
<p>The mode of string-storage described here refers to plaintext storage without compression.&#160;<a class="footnote-backref" href="#fnref:36" title="Jump back to footnote 36 in the text">&#8617;</a></p>
</li>
<li id="fn:37">
<p>The actual quote that this has been paraphrased from is:<br><blockquote>ESTRAGON: Perhaps he could dance first and think afterwards, if it isn’t too much to ask him.<br>
VLADIMIR: [To  POZZO.] Would that be possible?<br>
POZZO: By all means, nothing simpler. It’s the natural order."</blockquote>
Samuel Beckett, ‘Waiting for Godot (1956)’, in <em>The Complete Dramatic Works</em>, [^New edition] (Faber and Faber, 2006), 7–88.&#160;<a class="footnote-backref" href="#fnref:37" title="Jump back to footnote 37 in the text">&#8617;</a></p>
</li>
<li id="fn:38">
<p>Richard Moore, <em>Computer And The Mind Of Man: Logic By Machine</em> (KQED-TV, National Educational Television, 1962), https://archive.org/details/Logic_by_Machine. The Prelinger Archives are a public-domain archive of both professional and home-made films from the USA, available from http://www.archive.org. Footage from this archive is also presented in Chapter 3, in <em>24fps Psycho</em>.&#160;<a class="footnote-backref" href="#fnref:38" title="Jump back to footnote 38 in the text">&#8617;</a></p>
</li>
<li id="fn:39">
<p>Ibid.&#160;<a class="footnote-backref" href="#fnref:39" title="Jump back to footnote 39 in the text">&#8617;</a></p>
</li>
<li id="fn:40">
<p>One limitation of the DwH grant was that we were not able to pay performers. I am immensely grateful to all who took part; credits can be found in the appendix.&#160;<a class="footnote-backref" href="#fnref:40" title="Jump back to footnote 40 in the text">&#8617;</a></p>
</li>
<li id="fn:41">
<p>See the following articles in MIT Technology Review: Emerging Technology from the arXiv, ‘How Google Converted Language Translation Into a Problem of Vector Space Mathematics’, <em>MIT Technology Review</em>, 25 September 2013, https://www.technologyreview.com/s/519581/how-google-converted-language-translation-into-a-problem-of-vector-space-mathematics/; Emerging Technology from the arXiv, ‘How Google “Translates” Pictures into Words Using Vector Space Mathematics’, <em>MIT Technology Review</em>, 1 December 2014, https://www.technologyreview.com/s/532886/how-google-translates-pictures-into-words-using-vector-space-mathematics/.&#160;<a class="footnote-backref" href="#fnref:41" title="Jump back to footnote 41 in the text">&#8617;</a></p>
</li>
<li id="fn:42">
<p>Tomas Mikolov et al., ‘Efficient Estimation of Word Representations in Vector Space’, <em>arXiv:1301.3781 [^Cs]</em>, 16 January 2013, http://arxiv.org/abs/1301.3781.&#160;<a class="footnote-backref" href="#fnref:42" title="Jump back to footnote 42 in the text">&#8617;</a></p>
</li>
<li id="fn:43">
<p>Ibid.&#160;<a class="footnote-backref" href="#fnref:43" title="Jump back to footnote 43 in the text">&#8617;</a></p>
</li>
<li id="fn:44">
<p>Emerging Technology from the arXiv, ‘How Google “Translates” Pictures into Words Using Vector Space Mathematics’.&#160;<a class="footnote-backref" href="#fnref:44" title="Jump back to footnote 44 in the text">&#8617;</a></p>
</li>
<li id="fn:45">
<p>Garnier, as quoted in Adrian Forty, <em>Words and Buildings: A Vocabulary of Modern Architecture</em> (New York: Thames &amp; Hudson, 2000), 90.&#160;<a class="footnote-backref" href="#fnref:45" title="Jump back to footnote 45 in the text">&#8617;</a></p>
</li>
<li id="fn:46">
<p>S. Gage, ‘The Wonder of Trivial Machines’, in <em>Protoarchitecture: Analogue and Digital Hybrids</em>, Architectural Design (John Wiley, 2008), 17.&#160;<a class="footnote-backref" href="#fnref:46" title="Jump back to footnote 46 in the text">&#8617;</a></p>
</li>
<li id="fn:47">
<p>Radim Řehůřek and Petr Sojka, ‘Software Framework for Topic Modelling with Large Corpora’, in <em>Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</em> (Valletta, Malta: ELRA, 2010), 45–50.&#160;<a class="footnote-backref" href="#fnref:47" title="Jump back to footnote 47 in the text">&#8617;</a></p>
</li>
<li id="fn:48">
<p>These included text from the Natural Language Toolkit, which includes over 50 large-scale corpora of written language; this includes the Brown Corpus, numerous online and news logs, the complete works of Shakespeare, and more. Steven Bird, Edward Loper, and Ewan Klein, <em>Natural Language Processing with Python</em>, First (Sebastopol, CA: O’Reilly Media, Inc., 2009).&#160;<a class="footnote-backref" href="#fnref:48" title="Jump back to footnote 48 in the text">&#8617;</a></p>
</li>
<li id="fn:49">
<p>Simon had been a dancer at the Opera since 1998, and was part of its new training programme to transition dancers into being choreographers.&#160;<a class="footnote-backref" href="#fnref:49" title="Jump back to footnote 49 in the text">&#8617;</a></p>
</li>
<li id="fn:50">
<p>Cristian Danescu-Niculescu-Mizil and Lillian Lee, ‘Chameleons in Imagined Conversations: A New Approach to Understanding Coordination of Linguistic Style in Dialogs.’, in <em>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011</em>, 2011. Available from https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html.&#160;<a class="footnote-backref" href="#fnref:50" title="Jump back to footnote 50 in the text">&#8617;</a></p>
</li>
<li id="fn:51">
<p>The screening process for words ensured they had appeared within the movie dialogues corpus at least five times.&#160;<a class="footnote-backref" href="#fnref:51" title="Jump back to footnote 51 in the text">&#8617;</a></p>
</li>
<li id="fn:52">
<p>Kevin Schlei, <em>GyrOSC</em>, version 2.4.2, iOS (Bit Shape Software, LLC., 2010), http://www.bitshapesoftware.com/instruments/gyrosc/. OSC stands for Open Sound Control. It is a lightweight protocol for streaming live messages across a network. Operationally, it is similar to MIDI, but offers a great deal of flexibility and is well documented.&#160;<a class="footnote-backref" href="#fnref:52" title="Jump back to footnote 52 in the text">&#8617;</a></p>
</li>
<li id="fn:53">
<p>Working with one male and one female dancer, both classically trained in ballet, revealed a difference in the modes of movement each one used.&#160;<a class="footnote-backref" href="#fnref:53" title="Jump back to footnote 53 in the text">&#8617;</a></p>
</li>
<li id="fn:54">
<p>Gordon Pask, <em>Conversation Theory: Applications in Education and Epistemology</em> (Amsterdam: Ellevier Scientific Publishlng Company, 1976).&#160;<a class="footnote-backref" href="#fnref:54" title="Jump back to footnote 54 in the text">&#8617;</a></p>
</li>
<li id="fn:55">
<p>Hexler, <em>TouchOSC</em>, version 1.9.8, iOS (Hexler Limited, 2008).&#160;<a class="footnote-backref" href="#fnref:55" title="Jump back to footnote 55 in the text">&#8617;</a></p>
</li>
<li id="fn:56">
<p>The only element that did not use OSC was the speech, which was generated from Max using a terminal command and the Macintosh’s built-in speech function, with a male and female voice signalling the dancers’ gender. Please note that using the dancers’ gender with associated voice was a decision based purely on the audience legibility of the show; the abstraction of the dancers’ moves being turned into speech was immediately obvious if the spoken voice reflected the gender of the dancer.&#160;<a class="footnote-backref" href="#fnref:56" title="Jump back to footnote 56 in the text">&#8617;</a></p>
</li>
<li id="fn:57">
<p>Radiohead, <em>A Moon Shaped Pool</em>, Digital download (XL Recordings Ltd, 2016); Johnny Greenwood, <em>There Will Be Blood</em>, Digital download (Nonesuch, 2007).&#160;<a class="footnote-backref" href="#fnref:57" title="Jump back to footnote 57 in the text">&#8617;</a></p>
</li>
<li id="fn:58">
<p>The mathing aerial views came courtesy of Opera Garnier’s symmetrical ironwork, which it transpires is perfect for affixing cameras to with only rudimentary measurements; and also having access to two identical BlackMagic cameras through the Pavillon.&#160;<a class="footnote-backref" href="#fnref:58" title="Jump back to footnote 58 in the text">&#8617;</a></p>
</li>
</ol>
</div>
                
              
              
                


              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../ch1/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Chapter 1: Diagramming the Absurd" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Chapter 1: Diagramming the Absurd
            </div>
          </div>
        </a>
      
      
        
        <a href="../ch3/" class="md-footer__link md-footer__link--next" aria-label="Next: Chapter 3: Noise and Difference" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Chapter 3: Noise and Difference
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.sections", "navigation.top", "toc.integrate"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../assets/javascripts/workers/search.409db549.min.js", "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.756773cc.min.js"></script>
      
    
  </body>
</html>